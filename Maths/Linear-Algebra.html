<!DOCTYPE html><html lang=en><meta http-equiv=content-type content="text/html; charset=UTF-8"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><title>Linear Algebra | Notes in Progress</title><meta property=og:title content=Linear algebra><meta name=description content=PageSubTitle><meta property=og:site_name content=Notes in progress><meta property=og:type content=article><script type=application/ld+json>
  {"@type":"BlogPosting","headline":"Linear Algebra","dateModified":"2019-02-09T00:00:00+00:00","datePublished":"2019-02-09T00:00:00+00:00","url":PageUrl,"mainEntityOfPage":{"@type":"WebPage","@id":PageUrl},"description":"PageSubTitle","@context":"http://schema.org"}</script><link rel=stylesheet href=https://febkor.github.io/NotesInProgress/static/main.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){for(var i,t=document.getElementsByClassName("math"),n=0;n<t.length;n++)i=t[n].firstChild,t[n].tagName=="SPAN"&&katex.render(i.data,t[n],{displayMode:t[n].classList.contains("display"),throwOnError:!1})})</script><header class=site-header role=banner><div class=wrapper><a class=site-title rel=author href=https://febkor.github.io/NotesInProgress/ >Notes in Progress</a><nav class=site-nav><input type=checkbox id=nav-trigger class=nav-trigger> <label for=nav-trigger></label></nav></div></header><main class=page-content aria-label=Content><div class=wrapper><article class="post h-entry" itemscope="" itemtype=http://schema.org/BlogPosting><header class=post-header><h1 class="post-title p-name" itemprop="name headline">Linear Algebra</h1></header><div class="post-content e-content" itemprop=articleBody><h1 id=basics>Basics</h1><h2 id=dot-products>Dot products</h2><p>Any line in <span class="math inline">\mathbb{R}^2</span> can be described by the dot product: all <span class="math inline">x</span> such that <span class="math inline">a \cdot x = c</span> where <span class="math inline">a, x</span> are vectors and <span class="math inline">c \in \mathbb{R}</span>.<p>The dot product is zero if and only if the two vectors are orthogonal. On <span class="math inline">\mathbb{R}^2</span>, this corresponds to two lines going through the origin that are perpendicular to each other.<p>We can think of <span class="math inline">a \cdot b</span> as measuring the extent to which <span class="math inline">a</span> and <span class="math inline">b</span> point in the same direction. The dot product is positive if <span class="math inline">a</span> and <span class="math inline">b</span> point in the same general direction, <span class="math inline">0</span> if they are perpendicular, and negative if they point in generally opposite directions.<h2 id=cross-products>Cross products</h2><p>The magnitude of the product <span class="math inline">| a \times b |</span> equals the area of a parallelogram with the vectors for sides.<h2 id=determinants>Determinants</h2><p>Can be computed from a square matrix. Intuitively represents the scaling factor of the transformation described by the matrix.<p>If a matrix <span class="math inline">A</span> has a non-zero determinant, then it is invertible.<h2 id=invertible-matrix>Invertible matrix</h2><p>Let <span class="math inline">A</span> be a square <span class="math inline">n \times n</span> matrix. Then the following are equivalent:<ul><li><span class="math inline">A</span> is invertible.<li><span class="math inline">\det(A) \neq 0</span>.<li>The columns of <span class="math inline">A</span> are linearly independent.<li>The columns of <span class="math inline">A</span> span <span class="math inline">R^n</span>.<li>The columns of <span class="math inline">A</span> are a basis in <span class="math inline">R^n</span>.<li>The rows of <span class="math inline">A</span> are linearly independent.<li>The rows of <span class="math inline">A</span> span <span class="math inline">R^n</span>.<li>The rows of <span class="math inline">A</span> are a basis in <span class="math inline">R^n</span>.<li>The reduced row echelon form of <span class="math inline">A</span> has a leading 1 in each row.<li>etc.</ul><p>If any of these above conditions do not hold, then <span class="math inline">A</span> maps the point to at most <span class="math inline">n-1</span> dimensions. For example, if <span class="math inline">A</span> was a <span class="math inline">3 \times 3</span> matrix, then <span class="math inline">Ax = y</span> would be in one or two dimensions (for example, a projection from 3-d space to 2-d). It is thus not possible to find an inverse transform to go from <span class="math inline">y</span> to <span class="math inline">x</span>.<h2 id=nullspace-kernel>Nullspace (kernel)</h2><p>The kernel of a linear map <span class="math inline">L: V \rightarrow W</span> between two vector spaces <span class="math inline">V</span> and <span class="math inline">W</span> is the set of all elements <span class="math inline">v \in V</span> such that <span class="math inline">L(v) = 0</span>. That is, <span class="math display">\ker(L) = \{v \in V | L(v) = 0\}.</span><p>The kernel <span class="math inline">L</span> is a linear subspace of the domain <span class="math inline">V</span>. The dimension of the kernel of <span class="math inline">L</span> is called the nullity.<p>The kernel of matrix can be computed by solving for <span class="math inline">x</span> in <span class="math display">A \cdot x = 0.</span><h2 id=rank>Rank</h2><p>The rank of a matrix <span class="math inline">A</span> is the dimension of the vector space generated (or spanned) by its columns. This is the same as the dimension of the space spanned by its rows.<p>Can be computed by counting the number of non-zero rows in its reduced row-echelon form.<p>The rank is also defined as the dimension of the image of <span class="math inline">L</span>. The rank-nullity theorem states<br> <span class="math display">\dim(\ker L) + \dim(\mathrm{im} L) = \dim(V).</span><h2 id=orthogonal-matrix>Orthogonal matrix</h2><p>An orthogonal matrix is a square matrix whose columns and rows are orthonormal vectors, i.e. <span class="math display">A^TA = AA^T = I.</span><p>That is, <span class="math inline">A^T = A^{-1}</span>.<p>Over the field of complex numbers, the matrix is orthogonal iff <span class="math display">A^*A = AA^* = I,</span> where <span class="math inline">^*</span> denotes the conjugate transpose.<p>These matrices preserve norms.<p>Theorems:<ul><li>If <span class="math inline">A</span> is orthogonal, then <span class="math inline">\det(A) = 1</span> or <span class="math inline">\det = -1</span>.</ul><h1 id=complex-vector-spaces>Complex vector spaces</h1><p>Scalars are complex numbers, and vectors and matrices consist of complex numbers. All the usual complex number rules and vector space rules apply.<p>Theorem:<ul><li>If a real matrix A, has complex evalues, then those evalues and evectors occur in conjugate pairs:<ul><li>if <span class="math inline">\lambda</span> is an evalue, then so is <span class="math inline">\bar{\lambda}</span><li>if <span class="math inline">x</span> is an evector, then so is <span class="math inline">\bar{x}</span></ul></ul><h2 id=hermitian-unitary-and-normal-matrices>Hermitian, Unitary and Normal matrices</h2><dl><dt>Conjugate Transpose<dd>If <span class="math inline">A</span> is a complex matrix, then the conjugate transpose of <span class="math inline">A</span> is <span class="math display">A^* = \bar{A}^T.</span></dl><p>The conjugate transpose has the following properties:<ul><li><span class="math inline">(A^*)^* = A</span><li><span class="math inline">(A \pm B)^* = (A^* \pm B^*)</span><li><span class="math inline">(kA)^* = \bar{k}A^*</span><li><span class="math inline">(AB)^* = B^*A^*</span></ul><p>A square complex matrix <span class="math inline">A</span> is said to be<ul><li>unitary: <span class="math inline">A^{-1} = A^*</span><li>Hermitian: <span class="math inline">A^* = A</span></ul><p>Unitary matrices generalize orthogonal matrices. Hermitian matrices generalize real symmetric matrices.<p>The eigenvalues of a Hermitian are real. For Hermitian <span class="math inline">A</span>, the eigenvectors from different eigenvalues are orthogonal. Example: if <span class="math inline">Av_1 = \lambda_1 v_1, Av_2 = \lambda_2 v_2</span>, then <span class="math inline">v_1 \cdot v_2 = 0.</span><p>If <span class="math inline">A</span> is a unitary matrix, then<ul><li><span class="math inline">\|Ax\| = \|x\|</span> for all <span class="math inline">x \in \mathbb{C}^n</span><li><span class="math inline">Ax\cdot Ay = x\cdot y</span> for all <span class="math inline">x, y \in \mathbb{C}^n</span></ul><h1 id=eigenvalue-decomposition>Eigenvalue decomposition</h1><p>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> matrix. A number <span class="math inline">\lambda</span> is an eigenvalue if there exists a nonzero solution vector <span class="math inline">x</span> such that <span class="math display">Ax = \lambda x.</span> Rearranging, we get <span class="math display">(A-\lambda I)x = 0.</span><p>To find a nonzero solution <span class="math inline">x</span> we must solve <span class="math display">\det(A-\lambda I) = 0.</span> Finding this determinant gives us an <span class="math inline">n</span>th-degree polynomial in <span class="math inline">\lambda</span>, called the characteristic equation of <span class="math inline">A</span>. The eigenvalues are the roots of this equation. The eigenvector corresponding to an eigenvalue <span class="math inline">\lambda</span> is obtained by applying Gaussian elimination to <span class="math inline">(A-\lambda I | 0).</span><p>Theorems (we assume here that <span class="math inline">A</span> is an <span class="math inline">n \times n</span> matrix):<ul><li>If <span class="math inline">A</span> is a triangular matrix (upper triangular, lower triangular, or diagonal) then the eigenvalues of A are the entries on the main diagonal<li><span class="math inline">A</span> is invertible if and only if <span class="math inline">\lambda = 0</span> is not an eigenvalue of <span class="math inline">A</span>.</ul><h2 id=diagonalization>Diagonalization</h2><p>The <span class="math inline">n \times n</span> matrix <span class="math inline">A</span> is diagonalizable if an <span class="math inline">n\times n</span> nonsingular (i.e. invertible) matrix <span class="math inline">P</span> can be found so that <span class="math display">P^{-1}AP = D</span> is a diagonal matrix. We can say <span class="math inline">A</span> is diagonalized, or <span class="math inline">P</span> diagonalises <span class="math inline">A</span>.<p>For a symmetric matrix <span class="math inline">A</span> with real entries, we can always find an orthogonal matrix <span class="math inline">P</span> that diagnolizes <span class="math inline">A</span>: <span class="math display">P^T A P = D</span><p>Theorems:<ul><li><span class="math inline">A</span> is diagonlizable if and only if it has <span class="math inline">n</span> linearly independent eigenvectors <span class="math inline">K_1, K_2, \ldots, K_n,</span>.<li>If an <span class="math inline">n \times n</span> matrix has <span class="math inline">n</span> distinct eigenvalues, then it is diagonalizable.</ul><p>If there are repeated eigenvalues then we need to find the eigenspace of the eigenvalues, <span class="math inline">E_\lambda</span>. This is the subspace of all <span class="math inline">x</span> such that <span class="math display">(A-\lambda I )x = 0.</span><p>An <span class="math inline">n \times n</span> matrix is diagonalizable if and only if for each e-value <span class="math inline">\lambda</span>, <span class="math display">\dim(E_\lambda) = \text{multiplicity of the e-value }\lambda.</span> The (algebraic) multiplicity of an evalue is the number of occurences of that value. The dimension of the e-space is simply the number of independent vectors in <span class="math inline">(A-\lambda I)</span>.<h2 id=methodology>Methodology</h2><p>To diagonalize a matrix <span class="math inline">A</span>:<ul><li>First find its eigenvalues.<li>If the eigenvalues are distinct then we know that <span class="math inline">A</span> can be diagonalized.<li>If there are repeated e-values, then we’re not sure if <span class="math inline">A</span> can be diagonalized. We can carry on and try find the evectors or we can do the following test (question might say what to do).<ul><li>Find the e-space for the repeated e-value<li>Find the dimension of the e-space<ul><li>the number of free variables describing the basis of the e-vector OR<li>number of columns minus the number of independent vectors in <span class="math inline">(A-\lambda I)</span></ul><li>If this dimension equals the multiplicity, then the matrix can be diagonalized, else stop.</ul><li>Find the eigenvectors for each eigenvalue.<ul><li>if the number of evectors is less than <span class="math inline">\dim(A)</span>, then the matrix cannot be diagonalized</ul></ul><h2 id=singular-value-decomposition>Singular value decomposition</h2><p>Extends diagnolization to general <span class="math inline">m\times n</span> matrices.<p>Eigenvalue decomposition<ul><li>symmetric <span class="math inline">n\times n</span> matrix <span class="math inline">A</span><li><span class="math inline">A=PDP^T</span><li><span class="math inline">P</span> is an <span class="math inline">n\times n</span> orthogonal matrix of eigenvectors of <span class="math inline">A</span><li><span class="math inline">D</span> is the diagonal matrix of eigenvalues</ul><p>Hessenburg decomposition<ul><li>asymmetric <span class="math inline">n\times n</span> matrix <span class="math inline">A</span><li><span class="math inline">A=PHP^T</span><li><span class="math inline">P</span> is an <span class="math inline">n\times n</span> orthogonal matrix<li><span class="math inline">H</span> is in upper Hessenburg form</ul><p>Schur decomposition<ul><li>asymmetric <span class="math inline">n\times n</span> matrix <span class="math inline">A</span> with real eigenvalues<li><span class="math inline">A=PSP^T</span><li><span class="math inline">P</span> is an <span class="math inline">n\times n</span> orthogonal matrix<li><span class="math inline">S</span> is upper triangular</ul><p>Orthogonal matrices don’t decrease numerical precision (i.e. they preserve round-off error).<dl><dt>Theorem<dd>If <span class="math inline">A</span> is an <span class="math inline">m\times n</span> matrix, then:</dl><ul><li><span class="math inline">A^TA</span> is orthogonally diagonalizable<li>the eigenvalues of <span class="math inline">A^TA</span> are <span class="math inline">\geq 0</span></ul><dl><dt>Definition<dd>Suppose <span class="math inline">A</span> is an <span class="math inline">m\times n</span> matrix, and that <span class="math inline">\lambda_i, i\in\{1,\ldots,n\}</span> are the eigenvalues of <span class="math inline">A^TA</span>, then <span class="math display">\sigma_i = \sqrt{\lambda_i}</span> are called singular values of <span class="math inline">A</span>. If <span class="math inline">A</span> is of rank <span class="math inline">k</span>, then there are only <span class="math inline">k</span> non-zero values on the diagonal.<dt>Theorem<dd>If <span class="math inline">A</span> is an <span class="math inline">m \times n</span> matrix, then <span class="math inline">A</span> can be written in the form <span class="math display">A= U\Sigma V^T,</span> where <span class="math inline">U,V</span> are orthogonal matrices, and <span class="math inline">\Sigma</span> is an <span class="math inline">m\times n</span> diagonal matrix of singular values of <span class="math inline">A</span>.</dl><h2 id=lu-decomposition>LU Decomposition</h2><dl><dt>Definition<dd>Factoring a matrix <span class="math inline">A</span> into the product <span class="math display">A = LU,</span> where <span class="math inline">L</span> is a lower triangular matrix and <span class="math inline">U</span> is an upper triangular matrix.</dl><p>Can be used to efficiently compute solutions <span class="math inline">x</span> in <span class="math inline">Ax=b</span>:<ul><li><span class="math inline">LUx=b</span><li>define <span class="math inline">y</span> such that <span class="math inline">Ux = y</span><li>solve for <span class="math inline">y</span> in <span class="math inline">Ly = b</span><li>then solve for <span class="math inline">x</span> in <span class="math inline">Ux=y</span></ul><p>LU decompositions are not unique. A square matrix <span class="math inline">A</span> can be factored into <span class="math inline">LU</span> form if it can be reduced to reduced row echelon form without row interchanges.<p>LDU factorization decomposes the matrix <span class="math inline">A</span> uniquely as <span class="math display">A = LDU,</span> where <span class="math inline">L, U</span> are lower and upper triangular matrix with 1s on the main diagonal, and <span class="math inline">D</span> is a diagonal matrix.<p>PLU-factorization is a related technique where the reduction to row echelon form is achieved by using the permuatation matrix <span class="math inline">P^{-1}</span> so that <span class="math inline">A = PLU</span>.<h1 id=inner-product-spaces>Inner Product Spaces</h1><dl><dt>Definition<dd>An inner product on a vector space <span class="math inline">V \subseteq \mathbb{R}^n</span> is a function that associates a real number <span class="math inline">\langle u,v \rangle</span> with every pair of vectors in <span class="math inline">V</span> such that <span class="math inline">\forall u,v,w \in V</span> and <span class="math inline">\forall k \in \mathbb{R}</span>:</dl><ul><li><span class="math inline">\langle u,v \rangle = \langle v,u \rangle</span><li><span class="math inline">\langle u+v,w \rangle = \langle u,w \rangle + \langle v,w \rangle</span><li><span class="math inline">\langle ku,v \rangle = k\langle u,v \rangle</span><li><span class="math inline">\langle v,v \rangle \geq 0</span>, and <span class="math inline">\langle v,v \rangle=0 \iff 0</span></ul><p>The dot product naturally satisfies these axioms. The Euclidean Space is the space with the dot product as the inner product.<p>The norm of <span class="math inline">v \in V</span> is denoted by <span class="math inline">\|v\| = \sqrt{\langle v,v \rangle}</span>. The distance between <span class="math inline">u</span> and <span class="math inline">v</span> would be <span class="math inline">\|u-v\|</span>.<p>The geometry of the vector space is thus dependent on the inner product used.<p>Examples of inner products on:<ul><li><span class="math inline">\mathbb{R}^n</span>: <span class="math inline">\langle u,v \rangle = Au \cdot Av = v^TA^TAu</span><li><span class="math inline">M_{nn}</span>: <span class="math inline">\langle U,V \rangle = \mathrm{tr}(U^TV)</span><li><span class="math inline">P_n</span>: <span class="math inline">\langle p,q \rangle = a_0b_0 + a_1b_1 + \ldots + a_nb_n</span>, where<ul><li><span class="math inline">p = a_0 + a_1x + \ldots + a_nx^n</span><li><span class="math inline">q = b_0 + b_1x + \ldots + b_nx^n</span></ul><li><span class="math inline">C[a,b]</span>: <span class="math inline">\langle f,g \rangle = \int_{a}^{b}f(x)g(x)dx</span></ul><h2 id=gram-schmidt-process>Gram-Schmidt Process</h2><p>Suppose <span class="math inline">S</span> is a set of two or more vectors in a real inner product space. <span class="math inline">S</span> is orthogonal if all pairs of distinct vectors are orthogonal. If each vector also has a norm of 1, then <span class="math inline">S</span> is orthonormal.<p>If <span class="math inline">S</span> orthogonal consists of nonzero vectors only, then <span class="math inline">S</span> is linearly independent. A basis consisting of orthonormal vectors is an orthonormal basis.<dl><dt>Orthogonal Basis Coordinate Theorem<dd>If <span class="math inline">S = \{v_1, v_2,\ldots,...,v_n\}</span> is an orthogonal basis for an inner product space <span class="math inline">V</span>, and if <span class="math inline">u \in V</span>, then <span class="math display">u = \frac{\langle u,v_1 \rangle}{\|v_1\|^2}v_1 + \frac{\langle u,v_2 \rangle}{\|v_2\|^2}v_2 + \ldots + \frac{\langle u,v_n \rangle}{\|v_n\|^2}v_n</span><dt>Projection Theorem<dd>If <span class="math inline">W</span> is a finite-dimensional subspace of an inner product space <span class="math inline">V</span>, then <span class="math inline">\forall u \in V</span>, <span class="math display">u = w_1 + w_2,</span> where <span class="math inline">w_1 \in W</span> (orthogonal projection of <span class="math inline">u</span> onto <span class="math inline">W</span>) and <span class="math inline">w_2 \in W^\perp</span>.</dl><p>A vector in <span class="math inline">V</span> can be orthogonally projected onto <span class="math inline">W</span> using the Orthogonal Basis Coordinate Theorem.<p>Every nonzero finite-dimensional inner product space has an orthonormal basis.<dl><dt>Gram-Schmidt Process:<dd>The process used to construct an orthogonal basis from a given basis.</dl><p>Example: The Legendre Polynomials are the polynomials in the orthogonal basis constructed from the standard basis <span class="math inline">\{ 1,x,x^2,\ldots,x^n\}</span> for the vector space <span class="math inline">P_n</span> with inner product <span class="math display">\langle p, q \rangle = \int_{-1}^{1}p(x)q(x) dx</span><h2 id=qr-decomposition>QR-Decomposition</h2><p>Uses:<ul><li>computing eigenvalues of large matrices<li>numerically stable matrix inversion</ul><dl><dt>Theorem<dd>If <span class="math inline">A</span> is an <span class="math inline">m\times n</span> matrix with linearly independent column vectors, then <span class="math inline">A</span> can be factored as <span class="math display">A=QR</span> where <span class="math inline">Q</span> is an <span class="math inline">m\times n</span> matrix with orthonormal column vectors (i.e. <span class="math inline">QQ^T=I</span>), and <span class="math inline">R</span> is an <span class="math inline">n\times n</span> invertible upper triangular matrix.</dl></div><a class=u-url href=Linear algebra hidden=""></a></article></div></main><footer class="site-footer h-card"><data class=u-url href=/ ></data><div class=wrapper><h2 class=footer-heading>Notes in Progress</h2><div class=footer-col-wrapper><div class="footer-col footer-col-1"><ul class=contact-list><li class=p-name>febkor<li><a class=u-email href=""></a></ul></div></div><div class="footer-col footer-col-3"><p></div></div></footer>