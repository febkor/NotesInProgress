<!DOCTYPE html><html lang=en><head><meta http-equiv=content-type content="text/html; charset=UTF-8"><meta name=viewport content="width=device-width, initial-scale=1"><title>Linear Algebra | Notes in Progress</title><meta property=og:title content=Linear algebra><meta property=og:site_name content=Notes in progress><meta property=og:type content=article><script type=application/ld+json>
  {"@type":"BlogPosting","headline":"Notes in Progress","dateModified":"2019-02-09T00:00:00+00:00","datePublished":"2019-02-09T00:00:00+00:00","url":https://febkor.github.io/NotesInProgress,"mainEntityOfPage":{"@type":"WebPage","@id":https://febkor.github.io/NotesInProgress},"description":"Notes","@context":"http://schema.org"}</script><link rel=stylesheet href=https://febkor.github.io/NotesInProgress/static/main.css><link rel="shortcut icon" type=image/x-icon href=https://febkor.github.io/NotesInProgress/static/favicon.ico><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded", function () {
    var mathElements = document.getElementsByClassName("math");
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") { katex.render(texText.data, mathElements[i], { displayMode: mathElements[i].classList.contains("display"), throwOnError: false } );
    }}});</script></head><body><header class=site-header role=banner><div class=wrapper><a class=site-title rel=author href=https://febkor.github.io/NotesInProgress>Notes in Progress</a><nav class=site-nav><input type=checkbox id=nav-trigger class=nav-trigger><label for=nav-trigger></label></nav></div></header><main class=page-content aria-label=Content><div class=wrapper><article class="post h-entry" itemscope itemtype=http://schema.org/BlogPosting><header class=post-header><h1 class="post-title p-name" itemprop="name headline">Linear Algebra</h1></header><div class="post-content e-content" itemprop=articleBody><h1 id=basics>Basics</h1><h2 id=dot-products>Dot products</h2><p>Any line in <span class="math inline">\mathbb{R}^2</span> can be described by the dot product: all <span class="math inline">x</span> such that <span class="math inline">a \cdot x = c</span> where <span class="math inline">a, x</span> are vectors and <span class="math inline">c \in \mathbb{R}</span>.</p><p>The dot product is zero if and only if the two vectors are orthogonal. On <span class="math inline">\mathbb{R}^2</span>, this corresponds to two lines going through the origin that are perpendicular to each other.</p><p>We can think of <span class="math inline">a \cdot b</span> as measuring the extent to which <span class="math inline">a</span> and <span class="math inline">b</span> point in the same direction. The dot product is positive if <span class="math inline">a</span> and <span class="math inline">b</span> point in the same general direction, <span class="math inline">0</span> if they are perpendicular, and negative if they point in generally opposite directions.</p><h2 id=cross-products>Cross products</h2><p>The magnitude of the product <span class="math inline">| a \times b |</span> equals the area of a parallelogram with the vectors for sides.</p><h2 id=determinants>Determinants</h2><p>Can be computed from a square matrix. Intuitively represents the scaling factor of the transformation described by the matrix.</p><p>If a matrix <span class="math inline">A</span> has a non-zero determinant, then it is invertible.</p><h2 id=invertible-matrix>Invertible matrix</h2><p>Let <span class="math inline">A</span> be a square <span class="math inline">n \times n</span> matrix. Then the following are equivalent:</p><ul><li><span class="math inline">A</span> is invertible.</li><li><span class="math inline">\det(A) \neq 0</span>.</li><li>The columns of <span class="math inline">A</span> are linearly independent.</li><li>The columns of <span class="math inline">A</span> span <span class="math inline">R^n</span>.</li><li>The columns of <span class="math inline">A</span> are a basis in <span class="math inline">R^n</span>.</li><li>The rows of <span class="math inline">A</span> are linearly independent.</li><li>The rows of <span class="math inline">A</span> span <span class="math inline">R^n</span>.</li><li>The rows of <span class="math inline">A</span> are a basis in <span class="math inline">R^n</span>.</li><li>The reduced row echelon form of <span class="math inline">A</span> has a leading 1 in each row.</li><li>etc.</li></ul><p>If any of these above conditions do not hold, then <span class="math inline">A</span> maps the point to at most <span class="math inline">n-1</span> dimensions. For example, if <span class="math inline">A</span> was a <span class="math inline">3 \times 3</span> matrix, then <span class="math inline">Ax = y</span> would be in one or two dimensions (for example, a projection from 3-d space to 2-d). It is thus not possible to find an inverse transform to go from <span class="math inline">y</span> to <span class="math inline">x</span>.</p><h2 id=nullspace-kernel>Nullspace (kernel)</h2><p>The kernel of a linear map <span class="math inline">L: V \rightarrow W</span> between two vector spaces <span class="math inline">V</span> and <span class="math inline">W</span> is the set of all elements <span class="math inline">v \in V</span> such that <span class="math inline">L(v) = 0</span>. That is, <span class="math display">\ker(L) = \{v \in V | L(v) = 0\}.</span></p><p>The kernel <span class="math inline">L</span> is a linear subspace of the domain <span class="math inline">V</span>. The dimension of the kernel of <span class="math inline">L</span> is called the nullity.</p><p>The kernel of matrix can be computed by solving for <span class="math inline">x</span> in <span class="math display">A \cdot x = 0.</span></p><h2 id=rank>Rank</h2><p>The rank of a matrix <span class="math inline">A</span> is the dimension of the vector space generated (or spanned) by its columns. This is the same as the dimension of the space spanned by its rows.</p><p>Can be computed by counting the number of non-zero rows in its reduced row-echelon form.</p><p>The rank is also defined as the dimension of the image of <span class="math inline">L</span>. The rank-nullity theorem states<br><span class="math display">\dim(\ker L) + \dim(\mathrm{im} L) = \dim(V).</span></p><h2 id=orthogonal-matrix>Orthogonal matrix</h2><p>An orthogonal matrix is a square matrix whose columns and rows are orthonormal vectors, i.e. <span class="math display">A^TA = AA^T = I.</span></p><p>That is, <span class="math inline">A^T = A^{-1}</span>.</p><p>Over the field of complex numbers, the matrix is orthogonal iff <span class="math display">A^*A = AA^* = I,</span> where <span class="math inline">^*</span> denotes the conjugate transpose.</p><p>These matrices preserve norms.</p><p>Theorems:</p><ul><li>If <span class="math inline">A</span> is orthogonal, then <span class="math inline">\det(A) = 1</span> or <span class="math inline">\det = -1</span>.</li></ul><h1 id=complex-vector-spaces>Complex vector spaces</h1><p>Scalars are complex numbers, and vectors and matrices consist of complex numbers. All the usual complex number rules and vector space rules apply.</p><p>Theorem:</p><ul><li>If a real matrix A, has complex evalues, then those evalues and evectors occur in conjugate pairs: <ul><li>if <span class="math inline">\lambda</span> is an evalue, then so is <span class="math inline">\bar{\lambda}</span></li><li>if <span class="math inline">x</span> is an evector, then so is <span class="math inline">\bar{x}</span></li></ul></li></ul><h2 id=hermitian-unitary-and-normal-matrices>Hermitian, Unitary and Normal matrices</h2><dl><dt>Conjugate Transpose</dt><dd>If <span class="math inline">A</span> is a complex matrix, then the conjugate transpose of <span class="math inline">A</span> is <span class="math display">A^* = \bar{A}^T.</span></dd></dl><p>The conjugate transpose has the following properties:</p><ul><li><span class="math inline">(A^*)^* = A</span></li><li><span class="math inline">(A \pm B)^* = (A^* \pm B^*)</span></li><li><span class="math inline">(kA)^* = \bar{k}A^*</span></li><li><span class="math inline">(AB)^* = B^*A^*</span></li></ul><p>A square complex matrix <span class="math inline">A</span> is said to be</p><ul><li>unitary: <span class="math inline">A^{-1} = A^*</span></li><li>Hermitian: <span class="math inline">A^* = A</span></li></ul><p>Unitary matrices generalize orthogonal matrices. Hermitian matrices generalize real symmetric matrices.</p><p>The eigenvalues of a Hermitian are real. For Hermitian <span class="math inline">A</span>, the eigenvectors from different eigenvalues are orthogonal. Example: if <span class="math inline">Av_1 = \lambda_1 v_1, Av_2 = \lambda_2 v_2</span>, then <span class="math inline">v_1 \cdot v_2 = 0.</span></p><p>If <span class="math inline">A</span> is a unitary matrix, then</p><ul><li><span class="math inline">\|Ax\| = \|x\|</span> for all <span class="math inline">x \in \mathbb{C}^n</span></li><li><span class="math inline">Ax\cdot Ay = x\cdot y</span> for all <span class="math inline">x, y \in \mathbb{C}^n</span></li></ul><h1 id=eigenvalue-decomposition>Eigenvalue decomposition</h1><p>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> matrix. A number <span class="math inline">\lambda</span> is an eigenvalue if there exists a nonzero solution vector <span class="math inline">x</span> such that <span class="math display">Ax = \lambda x.</span> Rearranging, we get <span class="math display">(A-\lambda I)x = 0.</span></p><p>To find a nonzero solution <span class="math inline">x</span> we must solve <span class="math display">\det(A-\lambda I) = 0.</span> Finding this determinant gives us an <span class="math inline">n</span>th-degree polynomial in <span class="math inline">\lambda</span>, called the characteristic equation of <span class="math inline">A</span>. The eigenvalues are the roots of this equation. The eigenvector corresponding to an eigenvalue <span class="math inline">\lambda</span> is obtained by applying Gaussian elimination to <span class="math inline">(A-\lambda I | 0).</span></p><p>Theorems (we assume here that <span class="math inline">A</span> is an <span class="math inline">n \times n</span> matrix):</p><ul><li>If <span class="math inline">A</span> is a triangular matrix (upper triangular, lower triangular, or diagonal) then the eigenvalues of A are the entries on the main diagonal</li><li><span class="math inline">A</span> is invertible if and only if <span class="math inline">\lambda = 0</span> is not an eigenvalue of <span class="math inline">A</span>.</li></ul><h2 id=diagonalization>Diagonalization</h2><p>The <span class="math inline">n \times n</span> matrix <span class="math inline">A</span> is diagonalizable if an <span class="math inline">n\times n</span> nonsingular (i.e.&#xA0;invertible) matrix <span class="math inline">P</span> can be found so that <span class="math display">P^{-1}AP = D</span> is a diagonal matrix. We can say <span class="math inline">A</span> is diagonalized, or <span class="math inline">P</span> diagonalises <span class="math inline">A</span>.</p><p>For a symmetric matrix <span class="math inline">A</span> with real entries, we can always find an orthogonal matrix <span class="math inline">P</span> that diagnolizes <span class="math inline">A</span>: <span class="math display">P^T A P = D</span></p><p>Theorems:</p><ul><li><span class="math inline">A</span> is diagonlizable if and only if it has <span class="math inline">n</span> linearly independent eigenvectors <span class="math inline">K_1, K_2, \ldots, K_n,</span>.</li><li>If an <span class="math inline">n \times n</span> matrix has <span class="math inline">n</span> distinct eigenvalues, then it is diagonalizable.</li></ul><p>If there are repeated eigenvalues then we need to find the eigenspace of the eigenvalues, <span class="math inline">E_\lambda</span>. This is the subspace of all <span class="math inline">x</span> such that <span class="math display">(A-\lambda I )x = 0.</span></p><p>An <span class="math inline">n \times n</span> matrix is diagonalizable if and only if for each e-value <span class="math inline">\lambda</span>, <span class="math display">\dim(E_\lambda) = \text{multiplicity of the e-value }\lambda.</span> The (algebraic) multiplicity of an evalue is the number of occurences of that value. The dimension of the e-space is simply the number of independent vectors in <span class="math inline">(A-\lambda I)</span>.</p><h2 id=methodology>Methodology</h2><p>To diagonalize a matrix <span class="math inline">A</span>:</p><ul><li>First find its eigenvalues.</li><li>If the eigenvalues are distinct then we know that <span class="math inline">A</span> can be diagonalized.</li><li>If there are repeated e-values, then we&#x2019;re not sure if <span class="math inline">A</span> can be diagonalized. We can carry on and try find the evectors or we can do the following test (question might say what to do). <ul><li>Find the e-space for the repeated e-value</li><li>Find the dimension of the e-space <ul><li>the number of free variables describing the basis of the e-vector OR</li><li>number of columns minus the number of independent vectors in <span class="math inline">(A-\lambda I)</span></li></ul></li><li>If this dimension equals the multiplicity, then the matrix can be diagonalized, else stop.</li></ul></li><li>Find the eigenvectors for each eigenvalue. <ul><li>if the number of evectors is less than <span class="math inline">\dim(A)</span>, then the matrix cannot be diagonalized</li></ul></li></ul><h2 id=singular-value-decomposition>Singular value decomposition</h2><p>Extends diagnolization to general <span class="math inline">m\times n</span> matrices.</p><p>Eigenvalue decomposition</p><ul><li>symmetric <span class="math inline">n\times n</span> matrix <span class="math inline">A</span></li><li><span class="math inline">A=PDP^T</span></li><li><span class="math inline">P</span> is an <span class="math inline">n\times n</span> orthogonal matrix of eigenvectors of <span class="math inline">A</span></li><li><span class="math inline">D</span> is the diagonal matrix of eigenvalues</li></ul><p>Hessenburg decomposition</p><ul><li>asymmetric <span class="math inline">n\times n</span> matrix <span class="math inline">A</span></li><li><span class="math inline">A=PHP^T</span></li><li><span class="math inline">P</span> is an <span class="math inline">n\times n</span> orthogonal matrix</li><li><span class="math inline">H</span> is in upper Hessenburg form</li></ul><p>Schur decomposition</p><ul><li>asymmetric <span class="math inline">n\times n</span> matrix <span class="math inline">A</span> with real eigenvalues</li><li><span class="math inline">A=PSP^T</span></li><li><span class="math inline">P</span> is an <span class="math inline">n\times n</span> orthogonal matrix</li><li><span class="math inline">S</span> is upper triangular</li></ul><p>Orthogonal matrices don&#x2019;t decrease numerical precision (i.e.&#xA0;they preserve round-off error).</p><dl><dt>Theorem</dt><dd>If <span class="math inline">A</span> is an <span class="math inline">m\times n</span> matrix, then: </dd></dl><ul><li><span class="math inline">A^TA</span> is orthogonally diagonalizable</li><li>the eigenvalues of <span class="math inline">A^TA</span> are <span class="math inline">\geq 0</span></li></ul><dl><dt>Definition</dt><dd>Suppose <span class="math inline">A</span> is an <span class="math inline">m\times n</span> matrix, and that <span class="math inline">\lambda_i, i\in\{1,\ldots,n\}</span> are the eigenvalues of <span class="math inline">A^TA</span>, then <span class="math display">\sigma_i = \sqrt{\lambda_i}</span> are called singular values of <span class="math inline">A</span>. If <span class="math inline">A</span> is of rank <span class="math inline">k</span>, then there are only <span class="math inline">k</span> non-zero values on the diagonal. </dd><dt>Theorem</dt><dd>If <span class="math inline">A</span> is an <span class="math inline">m \times n</span> matrix, then <span class="math inline">A</span> can be written in the form <span class="math display">A= U\Sigma V^T,</span> where <span class="math inline">U,V</span> are orthogonal matrices, and <span class="math inline">\Sigma</span> is an <span class="math inline">m\times n</span> diagonal matrix of singular values of <span class="math inline">A</span>. </dd></dl><h2 id=lu-decomposition>LU Decomposition</h2><dl><dt>Definition</dt><dd>Factoring a matrix <span class="math inline">A</span> into the product <span class="math display">A = LU,</span> where <span class="math inline">L</span> is a lower triangular matrix and <span class="math inline">U</span> is an upper triangular matrix. </dd></dl><p>Can be used to efficiently compute solutions <span class="math inline">x</span> in <span class="math inline">Ax=b</span>:</p><ul><li><span class="math inline">LUx=b</span></li><li>define <span class="math inline">y</span> such that <span class="math inline">Ux = y</span></li><li>solve for <span class="math inline">y</span> in <span class="math inline">Ly = b</span></li><li>then solve for <span class="math inline">x</span> in <span class="math inline">Ux=y</span></li></ul><p>LU decompositions are not unique. A square matrix <span class="math inline">A</span> can be factored into <span class="math inline">LU</span> form if it can be reduced to reduced row echelon form without row interchanges.</p><p>LDU factorization decomposes the matrix <span class="math inline">A</span> uniquely as <span class="math display">A = LDU,</span> where <span class="math inline">L, U</span> are lower and upper triangular matrix with 1s on the main diagonal, and <span class="math inline">D</span> is a diagonal matrix.</p><p>PLU-factorization is a related technique where the reduction to row echelon form is achieved by using the permuatation matrix <span class="math inline">P^{-1}</span> so that <span class="math inline">A = PLU</span>.</p><h2 id=moorepenrose-inverse>Moore&#x2013;Penrose Inverse</h2><p>A generalization of the matrix inverse. Also referred to as the pseudoinverse.</p><p>The pseudoinverse is defined and unique for all matrices whose entries are real or complex numbers.</p><p>For <span class="math inline">A \in \mathbb{K}^{m\times n}</span>, a pseudoinverse of <span class="math inline">A</span> is defined as a matrix <span class="math inline">A^+ \in \mathbb{K}^{n\times m}</span> satisfying all of the following (the Moore&#x2013;Penrose conditions):</p><ul><li><span class="math inline">AA^+A = A</span></li><li><span class="math inline">A^+AA^+ = A^+</span></li><li><span class="math inline">(AA^+)^* = AA^+</span> i.e.&#xA0;is Hermitian</li><li><span class="math inline">(A^+A)^* = A^+A</span> i.e.&#xA0;is Hermitian</li></ul><p>Properties</p><ul><li>if <span class="math inline">A</span> is invertible, then <span class="math inline">A^+ = A^{-1}</span></li><li>when <span class="math inline">A</span> has linearly independent columns, then <span class="math inline">A^+</span> can be computed (using the left inverse) as <span class="math display">A^+ = (A^*A)^{-1}A^*.</span></li><li>when <span class="math inline">A</span> has linearly independent rows, then <span class="math inline">A^+</span> can be computed (using the right inverse) as <span class="math display">A^+ = A^*(A^*A)^{-1}.</span></li></ul><p>Calculated using the SVD: if <span class="math inline">A = U\Sigma V^*</span>, then <span class="math inline">A^+ = V\Sigma^+ U^*</span>.</p><p>It can be used to find the the least squares solution or the minimum norm solution to a system of linear equations with no unique solution.</p><h1 id=inner-product-spaces>Inner Product Spaces</h1><dl><dt>Definition</dt><dd>An inner product on a vector space <span class="math inline">V \subseteq \mathbb{R}^n</span> is a function that associates a real number <span class="math inline">\langle u,v \rangle</span> with every pair of vectors in <span class="math inline">V</span> such that <span class="math inline">\forall u,v,w \in V</span> and <span class="math inline">\forall k \in \mathbb{R}</span>: </dd></dl><ul><li><span class="math inline">\langle u,v \rangle = \langle v,u \rangle</span></li><li><span class="math inline">\langle u+v,w \rangle = \langle u,w \rangle + \langle v,w \rangle</span></li><li><span class="math inline">\langle ku,v \rangle = k\langle u,v \rangle</span></li><li><span class="math inline">\langle v,v \rangle \geq 0</span>, and <span class="math inline">\langle v,v \rangle=0 \iff 0</span></li></ul><p>The dot product naturally satisfies these axioms. The Euclidean Space is the space with the dot product as the inner product.</p><p>The norm of <span class="math inline">v \in V</span> is denoted by <span class="math inline">\|v\| = \sqrt{\langle v,v \rangle}</span>. The distance between <span class="math inline">u</span> and <span class="math inline">v</span> would be <span class="math inline">\|u-v\|</span>.</p><p>The geometry of the vector space is thus dependent on the inner product used.</p><p>Examples of inner products on:</p><ul><li><span class="math inline">\mathbb{R}^n</span>: <span class="math inline">\langle u,v \rangle = Au \cdot Av = v^TA^TAu</span></li><li><span class="math inline">M_{nn}</span>: <span class="math inline">\langle U,V \rangle = \mathrm{tr}(U^TV)</span></li><li><span class="math inline">P_n</span>: <span class="math inline">\langle p,q \rangle = a_0b_0 + a_1b_1 + \ldots + a_nb_n</span>, where <ul><li><span class="math inline">p = a_0 + a_1x + \ldots + a_nx^n</span></li><li><span class="math inline">q = b_0 + b_1x + \ldots + b_nx^n</span></li></ul></li><li><span class="math inline">C[a,b]</span>: <span class="math inline">\langle f,g \rangle = \int_{a}^{b}f(x)g(x)dx</span></li></ul><h2 id=gram-schmidt-process>Gram-Schmidt Process</h2><p>Suppose <span class="math inline">S</span> is a set of two or more vectors in a real inner product space. <span class="math inline">S</span> is orthogonal if all pairs of distinct vectors are orthogonal. If each vector also has a norm of 1, then <span class="math inline">S</span> is orthonormal.</p><p>If <span class="math inline">S</span> orthogonal consists of nonzero vectors only, then <span class="math inline">S</span> is linearly independent. A basis consisting of orthonormal vectors is an orthonormal basis.</p><dl><dt>Orthogonal Basis Coordinate Theorem</dt><dd>If <span class="math inline">S = \{v_1, v_2,\ldots,...,v_n\}</span> is an orthogonal basis for an inner product space <span class="math inline">V</span>, and if <span class="math inline">u \in V</span>, then <span class="math display">u = \frac{\langle u,v_1 \rangle}{\|v_1\|^2}v_1 + \frac{\langle u,v_2 \rangle}{\|v_2\|^2}v_2 + \ldots + \frac{\langle u,v_n \rangle}{\|v_n\|^2}v_n</span></dd><dt>Projection Theorem</dt><dd>If <span class="math inline">W</span> is a finite-dimensional subspace of an inner product space <span class="math inline">V</span>, then <span class="math inline">\forall u \in V</span>, <span class="math display">u = w_1 + w_2,</span> where <span class="math inline">w_1 \in W</span> (orthogonal projection of <span class="math inline">u</span> onto <span class="math inline">W</span>) and <span class="math inline">w_2 \in W^\perp</span>. </dd></dl><p>A vector in <span class="math inline">V</span> can be orthogonally projected onto <span class="math inline">W</span> using the Orthogonal Basis Coordinate Theorem.</p><p>Every nonzero finite-dimensional inner product space has an orthonormal basis.</p><dl><dt>Gram-Schmidt Process:</dt><dd>The process used to construct an orthogonal basis from a given basis. </dd></dl><p>Example: The Legendre Polynomials are the polynomials in the orthogonal basis constructed from the standard basis <span class="math inline">\{ 1,x,x^2,\ldots,x^n\}</span> for the vector space <span class="math inline">P_n</span> with inner product <span class="math display">\langle p, q \rangle = \int_{-1}^{1}p(x)q(x) dx</span></p><h2 id=qr-decomposition>QR-Decomposition</h2><p>Uses:</p><ul><li>computing eigenvalues of large matrices</li><li>numerically stable matrix inversion</li></ul><dl><dt>Theorem</dt><dd>If <span class="math inline">A</span> is an <span class="math inline">m\times n</span> matrix with linearly independent column vectors, then <span class="math inline">A</span> can be factored as <span class="math display">A=QR</span> where <span class="math inline">Q</span> is an <span class="math inline">m\times n</span> matrix with orthonormal column vectors (i.e.&#xA0;<span class="math inline">QQ^T=I</span>), and <span class="math inline">R</span> is an <span class="math inline">n\times n</span> invertible upper triangular matrix. </dd></dl></article></div></main><footer class="site-footer h-card"><data href=/ class=u-url></data><div class=wrapper><h2 class=footer-heading>Notes in Progress</h2><div class=footer-col-wrapper><div class="footer-col footer-col-1"><ul class=contact-list><li class=p-name>febkor</li></ul></div></div></div></div></footer></body></html>