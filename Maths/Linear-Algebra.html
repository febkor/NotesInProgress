<!DOCTYPE html><html lang=en><meta http-equiv=content-type content="text/html; charset=UTF-8"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><title>Linear Algebra | Notes in Progress</title><meta property=og:title content=Linear algebra><meta name=description content=PageSubTitle><meta property=og:site_name content=Notes in progress><meta property=og:type content=article><script type=application/ld+json>
  {"@type":"BlogPosting","headline":"Linear Algebra","dateModified":"2019-02-09T00:00:00+00:00","datePublished":"2019-02-09T00:00:00+00:00","url":PageUrl,"mainEntityOfPage":{"@type":"WebPage","@id":PageUrl},"description":"PageSubTitle","@context":"http://schema.org"}</script><link rel=stylesheet href=https://febkor.github.io/NotesInProgress/static/main.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){for(var i,t=document.getElementsByClassName("math"),n=0;n<t.length;n++)i=t[n].firstChild,t[n].tagName=="SPAN"&&katex.render(i.data,t[n],{displayMode:t[n].classList.contains("display"),throwOnError:!1})})</script><header class=site-header role=banner><div class=wrapper><a class=site-title rel=author href=https://febkor.github.io/NotesInProgress/ >Notes in Progress</a><nav class=site-nav><input type=checkbox id=nav-trigger class=nav-trigger> <label for=nav-trigger></label></nav></div></header><main class=page-content aria-label=Content><div class=wrapper><article class="post h-entry" itemscope="" itemtype=http://schema.org/BlogPosting><header class=post-header><h1 class="post-title p-name" itemprop="name headline">Linear Algebra</h1></header><div class="post-content e-content" itemprop=articleBody><p>Just some of my random notes.<h1 id=dot-products>Dot products</h1><p>Any line in <span class="math inline">\mathbb{R}^2</span> can be described by the dot product: all <span class="math inline">x</span> such that <span class="math inline">a \cdot x = c</span> where <span class="math inline">a, x</span> are vectors and <span class="math inline">c \in \mathbb{R}</span>.<p>The dot product is zero if and only if the two vectors are orthogonal. On R2, this corresponds two lines going through the origin that are perpendicular to each other.<p>We can think of <span class="math inline">a \cdot b</span> as measuring the extent to which <span class="math inline">a</span> and <span class="math inline">b</span> point in the same direction. The dot product is positive if <span class="math inline">a</span> and <span class="math inline">b</span> point in the same general direction, <span class="math inline">0</span> if they are perpendicular, and negative if they point in generally opposite directions.<h1 id=cross-products>Cross products</h1><p>The magnitude of the product <span class="math inline">| a \times b |</span> equals the area of a parallelogram with the vectors for sides.<h1 id=determinants>Determinants</h1><p>Can be computed from a square matrix. Intuitively represents the scaling factor of the transformation described by the matrix.<p>If a matrix <span class="math inline">A</span> has a non-zero determinant, then it is invertible.<h1 id=invertible-matrix>Invertible matrix</h1><p>Let <span class="math inline">A</span> be a square <span class="math inline">n \times n</span> matrix. Then the following are equivalent:<ul><li><span class="math inline">A</span> is invertible.<li><span class="math inline">\det(A) \neq 0</span>.<li>The columns of <span class="math inline">A</span> are linearly independent.<li>The columns of <span class="math inline">A</span> span <span class="math inline">R^n</span>.<li>The columns of <span class="math inline">A</span> are a basis in <span class="math inline">R^n</span>.<li>The rows of <span class="math inline">A</span> are linearly independent.<li>The rows of <span class="math inline">A</span> span <span class="math inline">R^n</span>.<li>The rows of <span class="math inline">A</span> are a basis in <span class="math inline">R^n</span>.<li>The reduced row echelon form of <span class="math inline">A</span> has a leading 1 in each row.<li>etc.</ul><p>If any of these above conditions do not hold, then <span class="math inline">A</span> maps the point to at most <span class="math inline">n-1</span> dimensions. For example, if <span class="math inline">A</span> was a <span class="math inline">3 \times 3</span> matrix, then <span class="math inline">Ax = y</span> would be in one or two dimensions (for example, a projection from 3-d space to 2-d). It is thus not possible to find an inverse transform to go from <span class="math inline">y</span> to <span class="math inline">x</span>.<h1 id=nullspace-kernel>Nullspace (kernel)</h1><p>The kernel of a linear map <span class="math inline">L: V \rightarrow W</span> between two vector spaces <span class="math inline">V</span> and <span class="math inline">W</span> is the set of all elements <span class="math inline">v \in V</span> such that <span class="math inline">L(v) = 0</span>. That is, <span class="math display">\ker(L) = \{v \in V | L(v) = 0\}.</span><p>The kernel <span class="math inline">L</span> is a linear subspace of the domain <span class="math inline">V</span>. The dimension of the kernel of <span class="math inline">L</span> is called the nullity.<p>The kernel of matrix can be computed by solving for <span class="math inline">x</span> in <span class="math display">A \cdot x = 0.</span><h1 id=rank>Rank</h1><p>The rank of a matrix <span class="math inline">A</span> is the dimension of the vector space generated (or spanned) by its columns. This is the same as the dimension of the space spanned by its rows.<p>Can be computed by counting the number of non-zero rows in its reduced row-echelon form.<p>The rank is also defined as the dimension of the image of <span class="math inline">L</span>. The rank-nullity theorem states<br> <span class="math display">\dim(\ker L) + \dim(\mathrm{im} L) = \dim(V).</span><h1 id=orthogonal-matrix>Orthogonal matrix</h1><p>An orthogonal matrix is a square matrix whose columns and rows are orthonormal vectors, i.e. <span class="math display">A^TA = AA^T = I.</span><p>That is, <span class="math inline">A^T = A^{-1}</span>.<p>Over the field of complex numbers, the matrix is orthogonal iff <span class="math display">A^*A = AA^* = I,</span> where <span class="math inline">^*</span> denotes the conjugate transpose.<p>These matrices preserve norms.<p>Theorems:<ul><li>If <span class="math inline">A</span> is orthogonal, then <span class="math inline">\det(A) = 1</span> or <span class="math inline">\det = -1</span>.</ul><h1 id=eigenvalue-decomposition>Eigenvalue decomposition</h1><p>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> matrix. A number <span class="math inline">\lambda</span> is an eigenvalue if there exists a nonzero solution vector <span class="math inline">x</span> such that <span class="math display">Ax = \lambda x.</span> Rearranging, we get <span class="math display">(A-\lambda I)x = 0.</span><p>To find a nonzero solution <span class="math inline">x</span> we must solve <span class="math display">\det(A-\lambda I) = 0.</span> Finding this determinant gives us an <span class="math inline">n</span>th-degree polynomial in <span class="math inline">\lambda</span>, called the characteristic equation of <span class="math inline">A</span>. The eigenvalues are the roots of this equation. The eigenvector corresponding to an eigenvalue <span class="math inline">\lambda</span> is obtained by applying Gaussian elimination to <span class="math inline">(A-\lambda I | 0).</span><p>Theorems (we assume here that <span class="math inline">A</span> is an <span class="math inline">n \times n</span> matrix):<ul><li>If <span class="math inline">A</span> is a triangular matrix (upper triangular, lower triangular, or diagonal) then the eigenvalues of A are the entries on the main diagonal<li><span class="math inline">A</span> is invertible if and only if <span class="math inline">\lambda = 0</span> is not an eigenvalue of <span class="math inline">A</span>.</ul><h1 id=diagonalization>Diagonalization</h1><p>The <span class="math inline">n \times n</span> matrix <span class="math inline">A</span> is diagonalizable if an <span class="math inline">n\times n</span> nonsingular (i.e.Â invertible) matrix <span class="math inline">P</span> can be found so that <span class="math display">P^{-1}AP = D</span> is a diagonal matrix. We can say <span class="math inline">A</span> is diagonalized, or <span class="math inline">P</span> diagonalises <span class="math inline">A</span>.<p>For a symmetric matrix <span class="math inline">A</span> with real entries, we can always find an orthogonal matrix <span class="math inline">P</span> that diagnolizes <span class="math inline">A</span>: <span class="math display">P^T A P = D</span><p>Theorems:<ul><li><span class="math inline">A</span> is diagonlizable if and only if it has <span class="math inline">n</span> linearly independent eigenvectors <span class="math inline">K_1, K_2, \ldots, K_n,</span>.<li>If an <span class="math inline">n \times n</span> matrix has <span class="math inline">n</span> distinct eigenvalues, then it is diagonalizable.</ul><p>If there are repeated eigenvalues then we need to find the eigenspace of the eigenvalues, <span class="math inline">E_\lambda</span>. This is the subspace of all <span class="math inline">x</span> such that <span class="math display">(A-\lambda I )x = 0.</span><p>An <span class="math inline">n \times n</span> matrix is diagonalizable if and only if for each e-value <span class="math inline">\lambda</span>, <span class="math display">\dim(E_\lambda) = \text{multiplicity of the e-value }\lambda.</span> The (algebraic) multiplicity of an evalue is the number of occurences of that value. The dimension of the e-space is simply the number of independent vectors in <span class="math inline">(A-\lambda I)</span>.<h2 id=methodology>Methodology</h2><p>To diagonalize a matrix <span class="math inline">A</span>:<ul><li>First find its eigenvalues.<li>If the eigenvalues are distinct then we know that <span class="math inline">A</span> can be diagonalized.<li>If there are repeated e-values, then weâre not sure if <span class="math inline">A</span> can be diagonalized. We can carry on and try find the evectors or we can do the following test (question might say what to do).<ul><li>Find the e-space for the repeated e-value<li>Find the dimension of the e-space<ul><li>the number of free variables describing the basis of the e-vector OR<li>number of columns minus the number of independent vectors in <span class="math inline">(A-\lambda I)</span></ul><li>If this dimension equals the multiplicity, then the matrix can be diagonalized, else stop.</ul><li>Find the eigenvectors for each eigenvalue.<ul><li>if the number of evectors is less than <span class="math inline">\dim(A)</span>, then the matrix cannot be diagonalized</ul><li>Write down <span class="math inline">P</span> (using the eigenvectors) and <span class="math inline">D</span> (with the eigenvalues) (donât worry about inverting <span class="math inline">P</span>)<li>Pat yourself on the back for a job well done</ul><h1 id=complex-vector-spaces>Complex vector spaces</h1><p>Scalars are complex numbers, and vectors and matrices consist of complex numbers. All the usual complex number rules and vector space rules apply.<p>Theorem:<ul><li>If a real matrix A, has complex evalues, then those evalues and evectors occur in conjugate pairs:<ul><li>if <span class="math inline">\lambda</span> is an evalue, then so is <span class="math inline">\bar{\lambda}</span><li>if <span class="math inline">x</span> is an evector, then so is <span class="math inline">\bar{x}</span></ul></ul><h1 id=singular-value-decomposition>Singular value decomposition</h1><p>â¦<h1 id=conic-sections>Conic Sections</h1><p>Quadratic form: <span class="math display">ax^2 + bxy + cy^2.</span><p>By diagonalizing this form, we can write this under a new set of axes which would simplify the form, and make it easier to identify.</div><a class=u-url href=Linear algebra hidden=""></a></article></div></main><footer class="site-footer h-card"><data class=u-url href=/ ></data><div class=wrapper><h2 class=footer-heading>Notes in Progress</h2><div class=footer-col-wrapper><div class="footer-col footer-col-1"><ul class=contact-list><li class=p-name>febkor<li><a class=u-email href=""></a></ul></div></div><div class="footer-col footer-col-3"><p></div></div></footer>