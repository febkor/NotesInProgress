<!DOCTYPE html><html lang=en><meta http-equiv=content-type content="text/html; charset=UTF-8"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1"><title>Numerical Methods | Notes in Progress</title><meta property=og:title content=Numerical methods><meta name=description content=PageSubTitle><meta property=og:site_name content=Notes in progress><meta property=og:type content=article><script type=application/ld+json>
  {"@type":"BlogPosting","headline":"Numerical Methods","dateModified":"2019-02-09T00:00:00+00:00","datePublished":"2019-02-09T00:00:00+00:00","url":PageUrl,"mainEntityOfPage":{"@type":"WebPage","@id":PageUrl},"description":"PageSubTitle","@context":"http://schema.org"}</script><link rel=stylesheet href=https://febkor.github.io/NotesInProgress/static/main.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){for(var i,t=document.getElementsByClassName("math"),n=0;n<t.length;n++)i=t[n].firstChild,t[n].tagName=="SPAN"&&katex.render(i.data,t[n],{displayMode:t[n].classList.contains("display"),throwOnError:!1})})</script><header class=site-header role=banner><div class=wrapper><a class=site-title rel=author href=https://febkor.github.io/NotesInProgress/ >Notes in Progress</a><nav class=site-nav><input type=checkbox id=nav-trigger class=nav-trigger> <label for=nav-trigger></label></nav></div></header><main class=page-content aria-label=Content><div class=wrapper><article class="post h-entry" itemscope="" itemtype=http://schema.org/BlogPosting><header class=post-header><h1 class="post-title p-name" itemprop="name headline">Numerical Methods</h1></header><div class="post-content e-content" itemprop=articleBody><h1 id=solving-nonlinear-equations>Solving Nonlinear Equations</h1><p>Here we look at the problem of root-finding.<p>Suppose we want to solve <span class="math display">f(x) = 0,</span> that is, we want <span class="math display">x = f^{-1}(0).</span><p>If our function is not of this form, we can often transform it so that it is.<p>Typical numerical methods for solving such equations are iterative and approach the true solution in the limit. We can stop once the result is close enough.<p>Stopping criteria<ul><li>absolute error: <span class="math display">|x_{n+1}-x_n| &lt; \epsilon</span><li>relative error <span class="math display">\frac{|x_{n+1}-x_n|}{|x_{n+1}|} &lt; \epsilon</span><li>function value <span class="math display">|f({n+1})| &lt; \epsilon</span></ul><p>Rootfinding problems are well-conditioned if the graph of <span class="math inline">f</span> strikes the axis with a nice large gradient (see lecture 8 slide 13).<p>If the problem is ill-conditioned then:<ul><li>the solution is sensitive to perturbations<ul><li>eg <span class="math inline">f(x) - k = 0</span> could give a very different solution <span class="math inline">x</span> for tiny changes in <span class="math inline">k</span>.</ul><li>likely slower convergence</ul><h2 id=bisection>Bisection</h2><p>A bracketing method. Involves dividing the interval into smaller and smaller intervals that contain the root.<ol type=1><li>Find interval <span class="math inline">[a,b]</span> where <span class="math inline">f(x)</span> changes sign<li>Compute <span class="math inline">f(c)</span> where <span class="math inline">c = (a+b)/2</span> is midpoint of interval<li>Compare signs of <span class="math inline">f(a),f(b),f(c)</span> and repeat with interval of half-size</ol><p>Pseudocode
<div class=sourceCode id=cb1><pre class="sourceCode python"><code class="sourceCode python"><a class=sourceLine id=cb1-1 title=1></a>
<a class=sourceLine id=cb1-2 title=2><span class=co># stop_criteria(...) returns true if the stopping critera are met</span></a>
<a class=sourceLine id=cb1-3 title=3></a>
<a class=sourceLine id=cb1-4 title=4>fa <span class=op>=</span> f(a)</a>
<a class=sourceLine id=cb1-5 title=5>fb <span class=op>=</span> f(b)</a>
<a class=sourceLine id=cb1-6 title=6></a>
<a class=sourceLine id=cb1-7 title=7><span class=cf>if</span> sign(fa) <span class=op>==</span> sign(fb):</a>
<a class=sourceLine id=cb1-8 title=8>    <span class=cf>raise</span> <span class=pp>Exception</span>(<span class=st>"The interval doesn't bracket the root"</span>)</a>
<a class=sourceLine id=cb1-9 title=9></a>
<a class=sourceLine id=cb1-10 title=10><span class=cf>while</span> <span class=kw>not</span> stop_criteria(...):</a>
<a class=sourceLine id=cb1-11 title=11>    <span class=co># bisect</span></a>
<a class=sourceLine id=cb1-12 title=12>    c <span class=op>=</span> (a <span class=op>+</span> b)<span class=op>/</span><span class=dv>2</span></a>
<a class=sourceLine id=cb1-13 title=13>    fc <span class=op>=</span> f(c)</a>
<a class=sourceLine id=cb1-14 title=14></a>
<a class=sourceLine id=cb1-15 title=15>    <span class=co># update interval</span></a>
<a class=sourceLine id=cb1-16 title=16>    <span class=cf>if</span> sign(fc) <span class=op>==</span> sign(fa): <span class=co># a,c are on same side of axis</span></a>
<a class=sourceLine id=cb1-17 title=17>        a <span class=op>=</span> c <span class=co># [c, b] becomes new [a, b]</span></a>
<a class=sourceLine id=cb1-18 title=18>        fa <span class=op>=</span> fc</a>
<a class=sourceLine id=cb1-19 title=19>    <span class=cf>else</span>: <span class=co># a, b are on same side of axis</span></a>
<a class=sourceLine id=cb1-20 title=20>        b <span class=op>=</span> c <span class=co># [a, c] becomes new [a, b]</span></a>
<a class=sourceLine id=cb1-21 title=21>        fb <span class=op>=</span> fc</a>
<a class=sourceLine id=cb1-22 title=22></a>
<a class=sourceLine id=cb1-23 title=23><span class=co># c should approach the root in the limit</span></a></code></pre></div><p>Positives:<ul><li>Guaranteed to converge (provided function is continuous and initial interval brackets a root)<li>Explicit error bound at each step<li>Needs no derivatives, only function values are used</ul><p>Negatives:<ul><li>Slow convergence<li>Not easily adaptable to two equations in two unknowns</ul><h2 id=false-position-regula-falsi>False Position (regula falsi)</h2><p><span class="math display">c = \frac{af(b) - bf(a)}{f(b)-f(a)}</span><h2 id=newton-raphson-method>Newton-Raphson Method</h2><p>A derivative method.<p><span class="math display">x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}</span><p>Positives:<ul><li>Fast convergence<li>Readily applicable to two equations in two unknowns</ul><p>Negatives:<ul><li>No guaranteed convergence<li>No explicit error bound<li>Needs a derivative</ul><h2 id=secant-method>Secant Method</h2><p>Replaces the derivative in the Newton iteration with the approximation <span class="math display">f'(x_n) \approx \frac{f(x_n)-f(x_{n-1})}{x_n - x_{n-1}}</span><p>Positives:<ul><li>does not require a derivative<li>only requires a single function evaluation <span class="math inline">f(x)</span> per iteration (Newton requires <span class="math inline">f(x)</span> and <span class="math inline">f'(x)</span>)</ul><p>Negatives:<ul><li>possibly slower convergence than Newton</ul><h2 id=fixed-point-iteration>Fixed-Point Iteration</h2><p>Used when we wish to solve a function of the form <span class="math display">f(x)=x.</span> We then use <span class="math display">x_{n+1} = f(x_n).</span><p>[Could also use <span class="math inline">x_{n+1} = 0.5(x_n + f(x_n)).</span> ]<p>Positives:<ul><li>Simple<li>Needs no derivative<li>Readily applicable to two equations in two unknowns</ul><p>Negatives:<ul><li>Typically slow convergence<li>No explicit error bound<li>No guaranteed convergence. Can diverge even with very good starting values.</ul><h2 id=systems-of-non-linear-equations>Systems of non-linear equations</h2><p>Let <span class="math inline">\underline{x}</span> now represents a vector of length <span class="math inline">m</span>: <span class="math inline">(x_1,\ldots,x_m).</span><p>Suppose we have <span class="math inline">m</span> equations <span class="math display">f_1(\underline{x}) = 0, f_2(\underline{x}) = 0,\ldots,f_m(\underline{x}) = 0.</span> We choose to represent this system of functions by <span class="math inline">F</span>.<p>So, our notation now reads <span class="math display">F(\underline{x}) = \underline{0}.</span><h3 id=newtons-method>Newton’s method</h3><p><span class="math display">\underline{x}_{n+1} = \underline{x}_n - [F'(\underline{x}_n)]^{-1} F(\underline{x}_n)</span><p><span class="math display">F'(\underline{x}_n) = J(\underline{x})</span> represents the Jacobian of <span class="math inline">F</span><p><span class="math inline">J^{-1} F(\underline{x}_n)</span> should be calculated using left-division if possible.<h3 id=broydens-method>Broyden’s method</h3><p>A quasi-Newton method for finding roots of a system of equations.<p>The multivariate Newton’s method uses the Jacobian matrix at every iteration (which is expensive to calculate). Broyden’s method computes the whole Jacobian only at the first iteration and does rank-one updates at other iterations.<h1 id=solving-systems-of-linear-equations>Solving systems of linear equations</h1><h2 id=gaussian-elimination>Gaussian elimination</h2><p>Suppose we wish to solve <span class="math inline">Ax = b.</span><ul><li>Lower triangular systems are solved by forward substitution.<li>Upper triangular systems are solved by back substitution.</ul><p>Gaussian elimination (GE) can be used to change an arbitrary matrix into an upper triangular form (from where you can use back substitution).<p>In GE, we should avoid zero or small pivots as this can lead to round-off error accumulating. GE would thus be an unstable algorithm. Zero or small pivots can be avoided by row interchanges (“pivoting”). The rows are interchanged so that the first column is arranged in decreasing order of absolute values (partial pivoting). If columns are interchanged as well, then this is called complete pivoting.<h2 id=lu-decomposition>LU decomposition</h2><p>GE (without pivoting) is equivalent to factoring <span class="math inline">A</span> into a lower-triangular (<span class="math inline">L</span>) and an upper-triangular (<span class="math inline">U</span>) matrix, <span class="math display">A=LU.</span><p>We can solve:<ul><li><span class="math inline">Ax = b</span><li><span class="math inline">LUx = b</span><li>let <span class="math inline">y = Ux \implies x = U^{-1}y</span> (back)<li><span class="math inline">Ly = b \implies y = L^{-1}b</span> (forward)</ul><p>With pivoting, the factorization becomes <span class="math display">PA = LU,</span> where <span class="math inline">P</span> is the permutation matrix that interchanges the rows. So, we have <span class="math display">Ax = b \implies PAx = Pb.</span><p>Thus, we have <span class="math inline">y = L^{-1}Pb</span>, and <span class="math inline">x = U^{-1}y</span><p>We first solve for <span class="math inline">y</span> in the above, then solve for <span class="math inline">x</span>. Because <span class="math inline">L</span> and <span class="math inline">U</span> are lower and upper triangular, we can use the (computationally fast) forward and back substitution, respectively. This means if we need to repeatedly solve a system involving <span class="math inline">A</span>, then we can compute the <span class="math inline">LU</span> factorization once and reuse each time. This makes the code faster.<p>The asymptotic complexity for computing <span class="math inline">A = LU</span> is <span class="math inline">\frac{2}{3}n^3</span> FLOPS. Forward/back solving with <span class="math inline">L</span> or <span class="math inline">U</span> only requires <span class="math inline">2n^2</span> FLOPS (and are thus faster).<p>The explicit computation of the inverse <span class="math inline">A^{-1}</span> is not recommended:<ul><li>about twice as slow as GE<li>more susceptible to round-off error than GE<li>if <span class="math inline">A</span> is sparse, some algorithms can exploit that to speed up calculations</ul><p>An <span class="math inline">n\times n</span> tridiagonal system can be solved using quickly using GE, or the Thomas/TDMA algorithm. A sparse left-division algorithm can be used if such an algorithm is not available.<h2 id=condition-number>Condition number</h2><p>Calculated as <span class="math display">\|A\|\|A^{-1}\|,</span> where <span class="math inline">\|A\|</span> is the norm of <span class="math inline">A</span>.<p>If the condition number is much greater than 1, then the system is ill-conditioned. This means that small relative changes in data may produce large relative changes in the solution.<p>As rule of thumb, if the condition number is of the order <span class="math inline">10^c</span>, then you should expect to lose <span class="math inline">c</span> digits of accuracy. Example, for float-point arithmetic (machine epsilon of approx 1e16), if the condition number of the system is 1e4, then you would only have 1e12 accuracy in the solution.<h1 id=curve-fitting>Curve Fitting</h1><h2 id=polynomials>Polynomials</h2><p>An <span class="math inline">n</span>-degree polynomial, written as <span class="math display">a_nx^n + a_{n-1}x^{n-1} \ldots + a_1x^1 + a_0</span> has <span class="math inline">n+1</span> coefficients.<p>Given <span class="math inline">m = n+1</span> points, we can fit a unique polynomial that goes through all the points.<p>If we have more than <span class="math inline">n+1</span> points, the system is overdetermined, and we have to find the line/curve of best fit.<p>If the polynomial degree <span class="math inline">n=1</span>, the problem of finding this best fit line is called linear regression.<p>We first parametrize the line as <span class="math display">y(x)=a_1x + a_0.</span> Given the co-ordinates in the plane <span class="math inline">(x_i, y_i)</span> of each of the <span class="math inline">m</span> points, we can write the error (or residual) of each point as the difference between the line and each point: <span class="math display">r_i = y(x_i) - y_i = \text{expected} - \text{actual}.</span> This can be visualized as the vertical distance between the line and the point.<p>To find the best fit line, we simply find the sum of squared errors: <span class="math display">\sum_{i}r_i^2 = \sum_{i}\left(y(x_i) - y_i\right)^2 = \sum_{i}(a_1x_i + a_0 -y_i)^2,</span> and minimize it w.r.t. <span class="math inline">a_0</span> and <span class="math inline">a_1</span>. That is, we try to find the line that minimizes the errors. To minimize, we differentiate wrt <span class="math inline">a_0</span> and <span class="math inline">a_1</span>, getting a system of equations called the normal equations.<p>The above can be extended to fit polynomials of any degree.
<div class=sourceCode id=cb2><pre class="sourceCode matlab"><code class="sourceCode matlab"><a class=sourceLine id=cb2-1 title=1>coefs = polyfit(x,y,n)    # n-degree polynomial</a>
<a class=sourceLine id=cb2-2 title=2>y_fit = polyval(coefs, x) # to evaluate the polynomial</a></code></pre></div><h2 id=linearization>Linearization</h2><p>Functions other than polynomials, such as exponentials and hyperbolas can be linearized. This means that these functions can be transformed into linear equation.<p><span class="math display">y = be^{mx} \implies \ln y = \ln b + mx</span> <span class="math display">y = \frac{1}{mx + b} \implies \frac{1}{y} = b + mx</span><h1 id=interpolation>Interpolation</h1><h2 id=lagrange-polynomials>Lagrange Polynomials</h2><p>These are used for polynomial interpolation. Given <span class="math inline">m=n+1</span> distinct points, the <span class="math inline">n</span>-degree Lagrange polynomial is defined as <span class="math display">P_n(x) = \sum_{i}L_i(x)y_i,</span> where <span class="math display">L_i(x) = \prod_{j\neq i} \frac{x-x_j}{x_i-x_j}.</span><p>Practise writing this for n=1,2,3!!! Answer a few questions from <a href=https://brilliant.org/wiki/lagrange-interpolation/ >Brilliant [link]</a>.<h3 id=alternatives>Alternatives</h3><ul><li>Newton Polynomial<li>Neville’s algorithm<li>Barycentric formula [know]<ul><li>a factorization of the Lagrange formula<li>can be more computationally efficient</ul></ul><h2 id=piecewise-interpolation>Piecewise Interpolation</h2><h3 id=linear-splines>Linear splines</h3><p>Given points/coordinates <span class="math display">(x_i, y_i), \quad i=1,2,\ldots,n</span> we can draw straight lines <span class="math display">f_i(x)=m_i(x)+c_i, \quad x_i\leq x\leq x_{i+1}, \quad i=1,2,\ldots,n-1</span> through each adjacent pair of points. This is called a linear spline, <span class="math inline">f(x)</span>. We can find the equation of each line using <span class="math display">f_{i}(x_i)=y_i \quad \text{and} \quad f_{i}(x_{i+1}) = y_{i+1}, \quad i=1,\ldots,n-1.</span><figure><img src=NumericalMethods_figures\LinearSpline.png alt="Linear Spline"><figcaption>Linear Spline</figcaption></figure><h3 id=cubic-splines>Cubic splines</h3><p>Given points/coordinates <span class="math display">(x_i, y_i), \quad i=1,2,\ldots,n</span> we can draw cubic functions <span class="math display">f_i(x)=a_ix^3 + b_ix^2 + c_ix + d_i, \quad x_i\leq x\leq x_{i+1}, \quad i=1,2,\ldots,n-1</span> through each pair of adjacent points. These define the cubic spline <span class="math inline">f(x)</span>.<figure><img src=NumericalMethods_figures\CubicSpline.png alt="Cubic Spline"><figcaption>Cubic Spline</figcaption></figure><p>We need to solve for <span class="math inline">(a_i, b_i, c_i, d_i)</span> to specify the whole of <span class="math inline">f(x)</span>. Because there are <span class="math inline">n-1</span> cubics, each with 4 parameters, there are <span class="math inline">4(n-1)</span> unknowns in total. We thus need the same number of equations:<ul><li><span class="math inline">f_{i}(x_i)=y_i</span> and <span class="math inline">f_{i}(x_{i+1}) = y_{i+1}</span> where <span class="math inline">i=1,\ldots,n-1</span>. This gives <span class="math inline">2(n-2)</span> equations.<li><span class="math inline">f_{i-1}'(x_i)=f_{i}'(x_i)</span> where <span class="math inline">i=2,\ldots,n-1</span>. This gives <span class="math inline">n-2</span> equations.<li><span class="math inline">f_{i-1}''(x_i)=f_{i}''(x_i)</span> where <span class="math inline">i=2,\ldots,n-1</span>. This gives <span class="math inline">n-2</span> equations.</ul><p>Since we need another 2 equations, we can use endpoint/boundary conditions:<ul><li>free/natural conditions: <span class="math inline">f''(x_1)=f''(x_n)=0</span>. A natural spline has the least total curvature of any possible smooth interpolant<li>clamped conditions: <span class="math inline">f'(x_1)=k_1</span> and <span class="math inline">f'(x_n)=k_2</span>, when you already have <span class="math inline">k_1, k_2</span> estimated/known<li>not-a-knot conditions: <span class="math inline">f^{(3)}(x_2) = f^{(3)}(x_{n-1})</span>, which ensure the cubics in the first two and last two intervals are the same</ul><p>The system of equations can now be solved. Can use backslash in MATLAB.<p>The cubic spline has the property that on the interval <span class="math inline">(x_1,x_n)</span>:<ul><li><span class="math inline">f(x)</span> is continuous (like for a linear interpolant)<li><span class="math inline">f'(x)</span> is continuous<li><span class="math inline">f''(x)</span> is continuous</ul><p>Be aware:<ul><li>one can simplify the system of equations by eliminating the <span class="math inline">b_i, c_i, d_i</span><li>this gives a tridiagonal system involving only <span class="math inline">a_i</span><li>this can be solved efficiently using the Thomas algorithm or sparse backslash<li>can use <code>spline</code> in MATLAB, which defaults to not-a-knot conditions<li>can also use <code>interp1</code>, which does 1-d interpolation. It gives options including linear, nearest, spline, Modified Akima</ul><h1 id=numerical-differentiation>Numerical Differentiation</h1><p>For some complicated functions, it’s not so easy to write down the derivative. We can numerically evaluate the derivative however.<h2 id=finite-differences>Finite Differences</h2><p>The derivative of a function <span class="math inline">f(x)</span> is defined as <span class="math display">f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}</span><p>Three simple finite difference formula are:<ul><li>forward: <span class="math inline">f'(x) \approx \frac{f(x+h)-f(x)}{h}</span><li>backward: <span class="math inline">f'(x) \approx \frac{f(x)-f(x-h)}{h}</span><li>central: <span class="math inline">f'(x) \approx \frac{f(x+h)-f(x-h)}{2h}</span></ul><p>For the second derivative, one could use: <span class="math display">f''(x) \approx \frac{f'(x+h/2)-f'(x-h/2)}{h} = \frac{f(x+h)+f(x-h)-2f(x)}{h^2}</span><p>There are more systematic methods for deriving finite difference formualas with Taylor series.<p>Lecture slides have more slides.<h2 id=lagrange-interpolation>Lagrange Interpolation</h2><h2 id=partial-differentiation>Partial Differentiation</h2><h1 id=numerical-integration>Numerical Integration</h1><p>Numerical integration is useful for integrals that don’t have analytic formulas or are too cumbersome to do by hand.<h2 id=rectangular>Rectangular</h2><p>This is simply a Riemann sum. One can use the left, mid or right endpoints of the intervals to evaluate the function.<h2 id=trapezium-rule>Trapezium Rule</h2><p>The area of a trapezium specified by the points <span class="math inline">(a,f(a))</span> and <span class="math inline">(b,f(b))</span> is <span class="math display">T(a,b) = \frac{1}{2}(b-a)(f(a) + f(b)).</span><p><span class="math display">\int_a^b f(x) dx = \sum_{i=1}^{n} \int_{x_{i-1}}^{x_i} f(x) dx \approx \sum_{i=1}^{n} T(x_{i-1},x_i).</span> If the <span class="math inline">x</span>-values are evenly-spaced, with <span class="math inline">h=(b-a)/n</span> we get <span class="math display">\int_a^b f(x) dx \approx h\left( f(a)/2 + \sum_{i=2}^{n} f(x_j) + f(b)/2 \right),</span> where <span class="math inline">x_j = a+(j-1)h</span>.<p>The trapezium rule converges with order 2, <span class="math inline">O(h^2)</span>.<h2 id=simpsons-rule>Simpson’s Rule</h2><p>Takes three points <span class="math inline">(a,f(a)), (b,f(b)), (c,f(c))</span> and fits a parabola through it. Uses the integral of the parabola <span class="math inline">P(x)</span> <span class="math display">\int_a^b f(x) dx \approx \int_a^b P(x) dx</span><p>With a step size <span class="math inline">h=(b-a)/2</span>, we get <span class="math display">\int_{a}^{b} P(x) \, dx =\tfrac{h}{3}\left[f(a) + 4f\left(\tfrac{a+b}{2}\right)+f(b)\right].</span> This is the Simpson’s <span class="math inline">\tfrac{1}{3}</span>-rule.<p>A <em>composite</em> Simpson rule is obtained by applying the basic rule across pairs of intervals. Assumes an <em>even</em> number of intervals. <span class="math display">\int_{a}^{b} P(x) \, dx =\tfrac{h}{3}\left[f(a) + 4 \sum_{j=2,4,\ldots}^{N}f(x_j) + 2 \sum_{j=3,5,\ldots}^{N-1}f(x_j) +f(b)\right].</span><p>Simpson’s rule converges with order 4, <span class="math inline">O(h^4)</span>.<h3 id=adaptive-simpsons-method>Adaptive Simpson’s method</h3><p>Uses an estimate of the error from calculating the definite integral using Simpson’s rule. If the error exceeds a specified tolerance, the interval of integration is subdivided in two and adaptive Simpson’s method is recursively called on each subinterval.<p>Since composite Simpson’s will subdivide even in places where the function is well-approximated by a parabola, the adaptive Simpson’s method can be much more efficient since it will use fewer function evaluations.<h2 id=gauss-integration>Gauss Integration</h2><p>The <span class="math inline">n</span>-point Gauss rule is defined as <span class="math display">\int_{-1}^{1} f(x) dx \approx \sum_{i=1}^{n} c_i f(x_i),</span> where <span class="math inline">c_i</span> are the weights and the <span class="math inline">x_i</span> are the nodes. The <span class="math inline">2n</span> weights and nodes need to be estimated. To estimate these, we require that the rule integrates <span class="math display">f(x)=x^{j},</span> exactly on <span class="math inline">[-1,1]</span> for <span class="math inline">j = 0,...,2n-1</span>. The <span class="math inline">x_i</span> are zeros of the Legendre polynomials, and thus this integration technique is also called Gauss-Legendre quadrature.<p>For example, the 2-point Gauss rule is defined as <span class="math display">\int_{-1}^{1} f(x) dx \approx f\left(\tfrac{-1}{\sqrt{3}}\right) + f\left(\tfrac{1}{\sqrt{3}}\right).</span> The 3-point Gauss rule is defined as <span class="math display">\int_{-1}^{1} f(x) dx \approx \tfrac{5}{9} f\left(-\sqrt{\tfrac{3}{5}}\right) + \tfrac{8}{9}f\left(0\right) + \tfrac{5}{9}f\left(\sqrt{\tfrac{3}{5}}\right).</span><p>An integral not on <span class="math inline">[-1,1]</span> can be tranformed into <span class="math display">\int_{a}^{b} f(x) dx =\int_{-1}^{1} f(t) dt,</span> by substituting <span class="math display">x=\frac{a+b+t(b-a)}{2},</span> with <span class="math inline">dx = \frac{b-a}{2} dt</span><h3 id=gausskronrod-quadrature>Gauss–Kronrod quadrature</h3><p>An adaptive variant of Gaussian quadrature, in which the evaluation points are chosen so that a more accurate approximation can be computed by re-using the the evaluation points from the previous iteration. It is an example of what is called a nested quadrature rule: for the same set of function evaluation points, it has two quadrature rules, one higher order and one lower order (the latter called an embedded rule). The difference between these two approximations is used to estimate the calculational error of the integration.<p>If the interval <span class="math inline">[a, b]</span> is subdivided, the usual Gauss evaluation points of the new subintervals never coincide with the previous evaluation points (except at the midpoint for odd numbers of evaluation points), and thus the integrand must be evaluated at every point. Gauss–Kronrod formulas are extensions of the Gauss quadrature formulas generated by adding <span class="math inline">n+1</span> points to an <span class="math inline">n</span>-point rule in such a way that the resulting rule is of order <span class="math inline">3n+1</span>; the corresponding Gauss rule is of order <span class="math inline">2n-1</span>.<h1 id=solving-ordinary-differential-equations>Solving Ordinary Differential Equations</h1><h2 id=initial-value-problems>Initial Value Problems</h2><p>Problem:<p>Find the function <span class="math inline">y(x)</span> such <span class="math display">y'(x)=f(x,y)</span> subject to the initial condition <span class="math inline">y_0 = y(x_0)</span> for a given function <span class="math inline">f</span>.<h3 id=eulers-method>Euler’s method</h3><p><span class="math display">y(x_{i+1}) \approx y(x_i) + hf(x_i,y(x_i)),</span> for <span class="math inline">i=0,1,2,\ldots</span>. This is a first order method, or Error = <span class="math inline">O(h)</span>.<h3 id=implicit-trapezium-rule>Implicit Trapezium Rule</h3><p><span class="math display">y(x_{i+1}) \approx y(x_i) + \tfrac{h}{2}\left( f(x_i,y(x_i)) + f(x_{i+1},y(x_{i+1})) \right).</span> This method requires us to know <span class="math inline">y(x_{i+1})</span> before estimating <span class="math inline">y(x_{i+1})</span>, which is a problem. We can try solve for this analytically or (more usually) numerically, or we could use an estimate for the <span class="math inline">y(x_{i+1})</span> on the right using the Euler method.<h3 id=modified-eulers-method>Modified Euler’s method</h3><p>We estimate <span class="math inline">y(x_{i+1})</span> from the implicit trapezium rule using one step of Euler’s method. <span class="math display">y(x_{i+1})^E = y(x_i) + hf(x_i,y(x_i)),</span> <span class="math display">y(x_{i+1}) \approx y(x_i) + \tfrac{h}{2}\left( f(x_i,y(x_i)) + f(x_{i+1},y(x_{i+1})^E) \right).</span> This is a second order method, or Error = <span class="math inline">O(h^2)</span>.<h3 id=midpoint-method>Midpoint method</h3><p><span class="math display">y(x_{i+0.5})^E = y(x_i) + \tfrac{h}{2}f(x_i,y(x_i))</span> <span class="math display">y(x_{i+1}) \approx y(x_i) + \tfrac{h}{2} f(x_i+\tfrac{h}{2},y(x_{i+0.5})^E).</span><p>The above are low order members of the Runge-Kutta methods.<p><code>ode45</code> in Matlab.<h2 id=boundary-value-problems>Boundary Value Problems</h2><p>Problem:<p>Find the function <span class="math inline">y(x)</span> such <span class="math display">y''(x) + f(x)y'(x) + g(x)y = h(x),</span> for <span class="math inline">a &lt; x &lt; b</span>, subject to boundary conditions <span class="math inline">y(a)=y_a, y(b)=y_b</span>.<p>Subdivide the interval <span class="math inline">[a,b]</span> into smaller intervals of width <span class="math inline">h</span>. Use central difference approximations of the derivatives <span class="math inline">y', y''</span>. Once the set of equations are simplified, it results in a tridiagonal system.<h1 id=solving-partial-differential-equations>Solving Partial Differential Equations</h1><p>Problem:<p>Find the function <span class="math inline">u(x,y)</span> such that, for example: <span class="math display">u_{xx}+u_{yy} = 0,</span> subject to some boundary conditions.<p>A two-variable PDE can be solved by using a derivative stencil (like the 5-point stencil) on a finite-difference grid. This produces a large sparse matrix which can be solved using different techniques:<ul><li>direct methods<li>iterative methods: e.g. Gauss-Seidel method, conjugate gradient method. These are better when the systems are huge, or only a low accuracy is needed.</ul></div><a class=u-url href=Numerical methods hidden=""></a></article></div></main><footer class="site-footer h-card"><data class=u-url href=/ ></data><div class=wrapper><h2 class=footer-heading>Notes in Progress</h2><div class=footer-col-wrapper><div class="footer-col footer-col-1"><ul class=contact-list><li class=p-name>febkor<li><a class=u-email href=""></a></ul></div></div><div class="footer-col footer-col-3"><p></div></div></footer>