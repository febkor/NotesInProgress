<!DOCTYPE html><html lang=en><head><meta http-equiv=content-type content="text/html; charset=UTF-8"><meta name=viewport content="width=device-width, initial-scale=1"><title>Numerical Methods | Notes in Progress</title><meta name=robots content=noindex><meta property=og:title content=Numerical methods><meta property=og:site_name content=Notes in progress><meta property=og:type content=article><script type=application/ld+json>
  {"@type":"BlogPosting","headline":"Notes in Progress","dateModified":"2019-02-09T00:00:00+00:00","datePublished":"2019-02-09T00:00:00+00:00","url":https://febkor.github.io/NotesInProgress,"mainEntityOfPage":{"@type":"WebPage","@id":https://febkor.github.io/NotesInProgress},"description":"Notes","@context":"http://schema.org"}</script><link rel=stylesheet href=https://febkor.github.io/NotesInProgress/static/main.css><link rel="shortcut icon" type=image/x-icon href=https://febkor.github.io/NotesInProgress/static/favicon.ico><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded", function () {
    var mathElements = document.getElementsByClassName("math");
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") { katex.render(texText.data, mathElements[i], { displayMode: mathElements[i].classList.contains("display"), throwOnError: false } );
    }}});</script></head><body><header class=site-header role=banner><div class=wrapper><a class=site-title rel=author href=https://febkor.github.io/NotesInProgress>Notes in Progress</a><nav class=site-nav><input type=checkbox id=nav-trigger class=nav-trigger><label for=nav-trigger></label></nav></div></header><main class=page-content aria-label=Content><div class=wrapper><article class="post h-entry" itemscope itemtype=http://schema.org/BlogPosting><header class=post-header><h1 class="post-title p-name" itemprop="name headline">Numerical Methods</h1></header><div class="post-content e-content" itemprop=articleBody><h1 id=solving-nonlinear-equations>Solving Nonlinear Equations</h1><p>Here we look at the problem of root-finding.</p><p>Suppose we want to solve <span class="math display">f(x) = 0,</span> that is, we want <span class="math display">x = f^{-1}(0).</span></p><p>If our function is not of this form, we can often transform it so that it is.</p><p>Typical numerical methods for solving such equations are iterative and approach the true solution in the limit. We can stop once the result is close enough.</p><p>Stopping criteria</p><ul><li>absolute error: <span class="math display">|x_{n+1}-x_n| &lt; \epsilon</span></li><li>relative error <span class="math display">\frac{|x_{n+1}-x_n|}{|x_{n+1}|} &lt; \epsilon</span></li><li>function value <span class="math display">|f({n+1})| &lt; \epsilon</span></li></ul><p>Rootfinding problems are well-conditioned if the graph of <span class="math inline">f</span> strikes the axis with a nice large gradient.</p><p>If the problem is ill-conditioned then:</p><ul><li>the solution is sensitive to perturbations <ul><li>eg <span class="math inline">f(x) - k = 0</span> could give a very different solution <span class="math inline">x</span> for tiny changes in <span class="math inline">k</span>.</li></ul></li><li>likely slower convergence</li></ul><h2 id=bisection>Bisection</h2><p>A bracketing method. Involves dividing the interval into smaller and smaller intervals that contain the root.</p><ol type=1><li>Find interval <span class="math inline">[a,b]</span> where <span class="math inline">f(x)</span> changes sign</li><li>Compute <span class="math inline">f(c)</span> where <span class="math inline">c = (a+b)/2</span> is midpoint of interval</li><li>Compare signs of <span class="math inline">f(a),f(b),f(c)</span> and repeat with interval of half-size</li></ol><p>Positives:</p><ul><li>Guaranteed to converge (provided function is continuous and initial interval brackets a root)</li><li>Explicit error bound at each step</li><li>Needs no derivatives, only function values are used</li></ul><p>Negatives:</p><ul><li>Slow convergence</li><li>Not easily adaptable to two equations in two unknowns</li></ul><h2 id=false-position-regula-falsi>False Position (regula falsi)</h2><p><span class="math display">c = \frac{af(b) - bf(a)}{f(b)-f(a)}</span></p><h2 id=newton-raphson-method>Newton-Raphson Method</h2><p>A derivative method.</p><p><span class="math display">x_{n+1} = x_n - \frac{f(x_n)}{f&#39;(x_n)} </span></p><p>Positives:</p><ul><li>Fast convergence</li><li>Readily applicable to two equations in two unknowns</li></ul><p>Negatives:</p><ul><li>No guaranteed convergence</li><li>No explicit error bound</li><li>Needs a derivative</li></ul><h2 id=secant-method>Secant Method</h2><p>Replaces the derivative in the Newton iteration with the approximation <span class="math display">f&#39;(x_n) \approx \frac{f(x_n)-f(x_{n-1})}{x_n - x_{n-1}}</span></p><p>Positives:</p><ul><li>does not require a derivative</li><li>only requires a single function evaluation <span class="math inline">f(x)</span> per iteration (Newton requires <span class="math inline">f(x)</span> and <span class="math inline">f&#39;(x)</span>)</li></ul><p>Negatives:</p><ul><li>possibly slower convergence than Newton</li></ul><h2 id=brents-method>Brent&#x2019;s Method</h2><p>Also known as the Brent-Dekker method. A hybrid method that combines the bisection method, the secant method and inverse quadratic interpolation. It tries to use the potentially fast-converging secant method or inverse quadratic interpolation if possible, but it falls back to the more robust bisection method if necessary.</p><p>Positives:</p><ul><li>as reliable as bisection</li><li>can be as quick as some of the less-reliable methods.</li></ul><h2 id=fixed-point-iteration>Fixed-Point Iteration</h2><p>Used when we wish to solve a function of the form <span class="math display">f(x)=x.</span> We then use <span class="math display">x_{n+1} = f(x_n).</span></p><p>[Could also use <span class="math inline">x_{n+1} = 0.5(x_n + f(x_n)).</span> ]</p><p>Positives:</p><ul><li>Simple</li><li>Needs no derivative</li><li>Readily applicable to two equations in two unknowns</li></ul><p>Negatives:</p><ul><li>Typically slow convergence</li><li>No explicit error bound</li><li>No guaranteed convergence. Can diverge even with very good starting values.</li></ul><h2 id=systems-of-non-linear-equations>Systems of Non-linear Equations</h2><p>Let <span class="math inline">\underline{x}</span> now represents a vector of length <span class="math inline">m</span>: <span class="math inline">(x_1,\ldots,x_m).</span></p><p>Suppose we have <span class="math inline">m</span> equations <span class="math display">f_1(\underline{x}) = 0, f_2(\underline{x}) = 0,\ldots,f_m(\underline{x}) = 0.</span> We choose to represent this system of functions by <span class="math inline">F</span>.</p><p>So, our notation now reads <span class="math display">F(\underline{x}) = \underline{0}.</span></p><h3 id=newtons-method>Newton&#x2019;s Method</h3><p><span class="math display">\underline{x}_{n+1} = \underline{x}_n - [F&#39;(\underline{x}_n)]^{-1} F(\underline{x}_n)</span></p><p><span class="math display">F&#39;(\underline{x}_n) = J(\underline{x})</span> represents the Jacobian of <span class="math inline">F</span></p><p><span class="math inline">J^{-1} F(\underline{x}_n)</span> should be calculated using left-division if possible.</p><h3 id=broydens-method>Broyden&#x2019;s Method</h3><p>A quasi-Newton method for finding roots of a system of equations.</p><p>The multivariate Newton&#x2019;s method uses the Jacobian matrix at every iteration (which is expensive to calculate). Broyden&#x2019;s method computes the whole Jacobian only at the first iteration and does rank-one updates at other iterations.</p><h1 id=solving-systems-of-linear-equations>Solving Systems of Linear Equations</h1><h2 id=gaussian-elimination>Gaussian Elimination</h2><p>Suppose we wish to solve <span class="math inline">Ax = b.</span></p><ul><li>Lower triangular systems are solved by forward substitution.</li><li>Upper triangular systems are solved by back substitution.</li></ul><p>Gaussian elimination (GE) can be used to change an arbitrary matrix into an upper triangular form (from where you can use back substitution).</p><p>In GE, we should avoid zero or small pivots as this can lead to round-off error accumulating. GE would thus be an unstable algorithm. Zero or small pivots can be avoided by row interchanges (&#x201C;pivoting&#x201D;). The rows are interchanged so that the first column is arranged in decreasing order of absolute values (partial pivoting). If columns are interchanged as well, then this is called complete pivoting.</p><h2 id=lu-decomposition>LU Decomposition</h2><p>GE (without pivoting) is equivalent to factoring <span class="math inline">A</span> into a lower-triangular (<span class="math inline">L</span>) and an upper-triangular (<span class="math inline">U</span>) matrix, <span class="math display">A=LU.</span></p><p>We can solve:</p><ul><li><span class="math inline">Ax = b</span></li><li><span class="math inline">LUx = b</span></li><li>let <span class="math inline">y = Ux \implies x = U^{-1}y</span> (back)</li><li><span class="math inline">Ly = b \implies y = L^{-1}b</span> (forward)</li></ul><p>With pivoting, the factorization becomes <span class="math display">PA = LU,</span> where <span class="math inline">P</span> is the permutation matrix that interchanges the rows. So, we have <span class="math display">Ax = b \implies PAx = Pb.</span></p><p>Thus, we have <span class="math inline">y = L^{-1}Pb</span>, and <span class="math inline">x = U^{-1}y</span></p><p>We first solve for <span class="math inline">y</span> in the above, then solve for <span class="math inline">x</span>. Because <span class="math inline">L</span> and <span class="math inline">U</span> are lower and upper triangular, we can use the (computationally fast) forward and back substitution, respectively. This means if we need to repeatedly solve a system involving <span class="math inline">A</span>, then we can compute the <span class="math inline">LU</span> factorization once and reuse each time. This makes the code faster.</p><p>The asymptotic complexity for computing <span class="math inline">A = LU</span> is <span class="math inline">\frac{2}{3}n^3</span> FLOPS. Forward/back solving with <span class="math inline">L</span> or <span class="math inline">U</span> only requires <span class="math inline">2n^2</span> FLOPS (and are thus faster).</p><p>The explicit computation of the inverse <span class="math inline">A^{-1}</span> is not recommended:</p><ul><li>about twice as slow as GE</li><li>more susceptible to round-off error than GE</li><li>if <span class="math inline">A</span> is sparse, some algorithms can exploit that to speed up calculations</li></ul><p>An <span class="math inline">n\times n</span> tridiagonal system can be solved using quickly using GE, or the Thomas/TDMA algorithm. A sparse left-division algorithm can be used if such an algorithm is not available.</p><h2 id=jacobi-iterative-method>Jacobi Iterative Method</h2><p>Iterative techniques are useful for solving large systems. Other methods might be computationally expensive.</p><p>To solve <span class="math inline">Ax = b</span>, we can split <span class="math inline">A</span> into <span class="math display">A = D + L + U,</span> i.e.&#xA0;a diagonal matrix, and lower and upper triangular matrices. The sytem can be written as <span class="math display">Dx = b - (L+U)x,</span> and if <span class="math inline">D^{-1}</span> exists (i.e.&#xA0;no 0s on the diagonal), then <span class="math display">x = D^{-1}[b - (L+U)x].</span></p><p>Now, given an initial guess <span class="math inline">x_0</span>, we can find approximations <span class="math display">x_k = D^{-1}[b - (L+U)x_{k-1}],</span> for <span class="math inline">k = 1,2,\ldots</span>. This iteration is guaranteed to converge if the system is diagonally dominant.</p><h2 id=gaussseidel-iterative-method>Gauss&#x2013;Seidel Iterative Method</h2><p>We can also write the system as <span class="math display">(D+L)x = b - Ux,</span> and if <span class="math inline">H = (D+L)^{-1}</span> exists then <span class="math display">x = H(b-Ux)</span> Now, given an initial guess <span class="math inline">x_0</span>, we can find approximations <span class="math display">x_k = H(b-Ux_{k-1})</span> for <span class="math inline">k = 1,2,\ldots</span>.</p><h2 id=condition-number>Condition Number</h2><p>Calculated as <span class="math display">\|A\|\|A^{-1}\|,</span> where <span class="math inline">\|A\|</span> is the norm of <span class="math inline">A</span>.</p><p>If the condition number is much greater than 1, then the system is ill-conditioned. This means that small relative changes in data may produce large relative changes in the solution.</p><p>As rule of thumb, if the condition number is of the order <span class="math inline">10^c</span>, then you should expect to lose <span class="math inline">c</span> digits of accuracy. Example, for float-point arithmetic (machine epsilon of approx 1e16), if the condition number of the system is 1e4, then you would only have 1e12 accuracy in the solution.</p><h1 id=curve-fitting>Curve Fitting</h1><h2 id=polynomials>Polynomials</h2><p>An <span class="math inline">n</span>-degree polynomial, written as <span class="math display">a_nx^n + a_{n-1}x^{n-1} \ldots + a_1x^1 + a_0</span> has <span class="math inline">n+1</span> coefficients.</p><p>Given <span class="math inline">m = n+1</span> points, we can fit a unique polynomial that goes through all the points.</p><p>If we have more than <span class="math inline">n+1</span> points, the system is overdetermined, and we have to find the line/curve of best fit.</p><p>If the polynomial degree <span class="math inline">n=1</span>, the problem of finding this best fit line is called linear regression.</p><p>We first parametrize the line as <span class="math display">y(x)=a_1x + a_0.</span> Given the co-ordinates in the plane <span class="math inline">(x_i, y_i)</span> of each of the <span class="math inline">m</span> points, we can write the error (or residual) of each point as the difference between the line and each point: <span class="math display">r_i = y(x_i) - y_i = \text{expected} - \text{actual}.</span> This can be visualized as the vertical distance between the line and the point.</p><p>To find the best fit line, we simply find the sum of squared errors: <span class="math display">\sum_{i}r_i^2 = \sum_{i}\left(y(x_i) - y_i\right)^2 = \sum_{i}(a_1x_i + a_0 -y_i)^2,</span> and minimize it w.r.t. <span class="math inline">a_0</span> and <span class="math inline">a_1</span>. That is, we try to find the line that minimizes the errors. To minimize, we differentiate wrt <span class="math inline">a_0</span> and <span class="math inline">a_1</span>, getting a system of equations called the normal equations.</p><p>The above can be extended to fit polynomials of any degree.</p><div class=sourceCode id=cb1><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id=cb1-1><a href=#cb1-1 aria-hidden=true tabindex=-1></a><span class=va>coefs</span> <span class=op>=</span> <span class=va>polyfit</span>(<span class=va>x</span><span class=op>,</span><span class=va>y</span><span class=op>,</span><span class=va>n</span>)    # <span class=va>n</span><span class=op>-</span><span class=va>degree</span> <span class=va>polynomial</span></span>
<span id=cb1-2><a href=#cb1-2 aria-hidden=true tabindex=-1></a><span class=va>y_fit</span> <span class=op>=</span> <span class=va>polyval</span>(<span class=va>coefs</span><span class=op>,</span> <span class=va>x</span>) # <span class=va>to</span> <span class=va>evaluate</span> <span class=va>the</span> <span class=va>polynomial</span></span></code></pre></div><h2 id=linearization>Linearization</h2><p>Functions other than polynomials, such as exponentials and hyperbolas can be linearized. This means that these functions can be transformed into linear equation.</p><p><span class="math display">y = be^{mx} \implies \ln y = \ln b + mx</span><span class="math display">y = \frac{1}{mx + b} \implies \frac{1}{y} = b + mx</span></p><h1 id=interpolation>Interpolation</h1><h2 id=polynomial-interpolation>Polynomial Interpolation</h2><p>The conditions that the polynomial have to satisfy can be expressed in different bases:</p><ul><li>monomial: <span class="math inline">p(x)=c_0 + c_1x + \cdots</span></li><li>Lagrange</li><li>Newton</li></ul><h3 id=lagrange-polynomials>Lagrange Polynomials</h3><p>These are used for polynomial interpolation. Given <span class="math inline">m=n+1</span> distinct points, the <span class="math inline">n</span>-degree Lagrange polynomial is defined as <span class="math display">P_n(x) = \sum_{i}L_i(x)y_i,</span> where <span class="math display">L_i(x) = \prod_{j\neq i} \frac{x-x_j}{x_i-x_j}.</span></p><p>Alternatives</p><ul><li>Newton Polynomial</li><li>Neville&#x2019;s algorithm</li><li>Barycentric formula [know] <ul><li>a factorization of the Lagrange formula</li><li>can be more computationally efficient</li></ul></li></ul><h3 id=newton-polynomials>Newton Polynomials</h3><p><span class="math display">P_n(x) = \sum_{i}a_i n_i(x),</span> with the Newton basis polynomials defined as <span class="math display">n_i(x) = \prod_{j=0}^{i-1} (x-x_j).</span></p><p>The coefficients are defined as <span class="math display">a_{j} =[y_{0},\ldots ,y_{j}],</span> where <span class="math inline">[\cdot]</span> is the notation for divided differences.</p><h3 id=hermite-interpolation>Hermite Interpolation</h3><p>If you have</p><ul><li>the first <span class="math inline">m</span> derivatives of the function <span class="math inline">f(x)</span></li><li>the function value at each of the <span class="math inline">n+1</span> node points <span class="math inline">x_0,\cdots,x_n</span></li></ul><p>That is, we have available a set of <span class="math inline">(n+1)(m+1)</span> values <span class="math inline">y_j^{(i)}=f^{(i)}(x_j), ;(j=0,\cdots,n,\;i=0,\cdots,m)</span></p><p>then the function can be interpolated by a polynomial of degree <span class="math inline">(n+1)(m+1)-1</span>: <span class="math display"> H_{(n+1)(m+1)-1}(x)=\sum_{k=0}^{(n+1)(m+1)-1} c_kx^k </span></p><p>The coefficients will be the solution of a system of equations that can be set up from the above. This is however computationally expensive. In practice, more efficient algorithms are used.</p><h2 id=piecewise-interpolation>Piecewise Interpolation</h2><p>Splines are piecewise polynomials.</p><h3 id=linear-splines>Linear Splines</h3><p>Given points/coordinates <span class="math display">(x_i, y_i), \quad i=1,2,\ldots,n</span> we can draw straight lines <span class="math display">f_i(x)=m_i(x)+c_i, \quad x_i\leq x\leq x_{i+1}, \quad i=1,2,\ldots,n-1</span> through each adjacent pair of points. This is called a linear spline, <span class="math inline">f(x)</span>. We can find the equation of each line using <span class="math display">f_{i}(x_i)=y_i \quad \text{and} \quad f_{i}(x_{i+1}) = y_{i+1}, \quad i=1,\ldots,n-1.</span></p><figure><img src=NumericalMethods_figures\LinearSpline.png alt="Linear Spline"><figcaption aria-hidden=true>Linear Spline</figcaption></figure><p>This has <span class="math inline">O(h^2)</span> error, where <span class="math inline">h</span> is the distance between interpolation points.</p><h3 id=cubic-splines>Cubic Splines</h3><p>Given points/coordinates <span class="math display">(x_i, y_i), \quad i=1,2,\ldots,n</span> we can draw cubic functions <span class="math display">f_i(x)=a_ix^3 + b_ix^2 + c_ix + d_i, \quad x_i\leq x\leq x_{i+1}, \quad i=1,2,\ldots,n-1</span> through each pair of adjacent points. These define the cubic spline <span class="math inline">f(x)</span>.</p><figure><img src=NumericalMethods_figures\CubicSpline.png alt="Cubic Spline"><figcaption aria-hidden=true>Cubic Spline</figcaption></figure><p>We need to solve for <span class="math inline">(a_i, b_i, c_i, d_i)</span> to specify the whole of <span class="math inline">f(x)</span>. Because there are <span class="math inline">n-1</span> cubics, each with 4 parameters, there are <span class="math inline">4(n-1)</span> unknowns in total. We thus need the same number of equations:</p><ul><li><span class="math inline">f_{i}(x_i)=y_i</span> and <span class="math inline">f_{i}(x_{i+1}) = y_{i+1}</span> where <span class="math inline">i=1,\ldots,n-1</span>. This gives <span class="math inline">2(n-2)</span> equations.</li><li><span class="math inline">f_{i-1}&#39;(x_i)=f_{i}&#39;(x_i)</span> where <span class="math inline">i=2,\ldots,n-1</span>. This gives <span class="math inline">n-2</span> equations.</li><li><span class="math inline">f_{i-1}&#39;&#39;(x_i)=f_{i}&#39;&#39;(x_i)</span> where <span class="math inline">i=2,\ldots,n-1</span>. This gives <span class="math inline">n-2</span> equations.</li></ul><p>Since we need another 2 equations, we can use endpoint/boundary conditions:</p><ul><li>free/natural: <span class="math inline">f&#39;&#39;(x_1)=f&#39;&#39;(x_n)=0</span>. A natural spline has the least total curvature of any possible smooth interpolant</li><li>clamped: <span class="math inline">f&#39;(x_1)=k_1</span> and <span class="math inline">f&#39;(x_n)=k_2</span>, when you already have <span class="math inline">k_1, k_2</span> estimated/known</li><li>not-a-knot: <span class="math inline">f^{(3)}(x_2) = f^{(3)}(x_{n-1})</span>, which ensure the cubics in the first two and last two intervals are the same</li><li>periodicity: <span class="math inline">f&#39;(x_1)=f&#39;(x_n), f&#39;&#39;(x_1)=f&#39;&#39;(x_n)</span></li></ul><p>The system of equations can now be solved. Can use backslash in MATLAB.</p><p>The cubic spline has the property that on the interval <span class="math inline">(x_1,x_n)</span>:</p><ul><li><span class="math inline">f(x)</span> is continuous (like for a linear interpolant)</li><li><span class="math inline">f&#39;(x)</span> is continuous</li><li><span class="math inline">f&#39;&#39;(x)</span> is continuous</li></ul><p>Be aware:</p><ul><li>one can simplify the system of equations by eliminating the <span class="math inline">b_i, c_i, d_i</span></li><li>this gives a tridiagonal system involving only <span class="math inline">a_i</span></li><li>this can be solved efficiently using the Thomas algorithm or sparse backslash</li><li>can use <code>spline</code> in MATLAB, which defaults to not-a-knot conditions</li><li>can also use <code>interp1</code>, which does 1-d interpolation. It gives options including linear, nearest, spline, Modified Akima</li></ul><h3 id=cubic-hermite-interpolation>Cubic Hermite Interpolation</h3><p>Piecewise cubic polynomials that satisfy Hermite interpolation conditions (PCHIP). The function values and derivatives are specified at each node point. These interpolants are in general not twice continuously differentiable.</p><p>A monotone variant is made possible by modifying the tangents <span class="math inline">m_{i}</span> to ensure the monotonicity of the resulting spline.</p><h4 id=bessel-splines>Bessel Splines</h4><p>A Hermite spline with the derivative at each node point <span class="math inline">x_i</span> chosen as that of the parabola that passes through <span class="math inline">(x_{i-1}, m_{i-1}), (x_i, m_i), (x_{i+1}, m_{i+1})</span><span class="math display">s_i = \frac{(x_i - x_{i-1})m_i + (x_{i+1} - x_i)m_{i-1}}{x_{i+1}-x_{i-1}},</span> with <span class="math inline">m_i = \frac{y_{i+1} - y_i}{x_{i+1} - x_i}</span>. This is the same as linearly interpolating <span class="math inline">m</span> between the midpoint of <span class="math inline">(x_{i-1}, x_i)</span> and <span class="math inline">(x_i ,x_{i+1})</span> and evaluating at <span class="math inline">x_i</span>.</p><p>This interpolation trades off some smoothness in favour of a &#x201C;dampening&#x201D; effect on the spline fit.</p><h3 id=generalized-splines>Generalized Splines</h3><ul><li>B-splines: these are splines that localise each part of the spline as much as possible; a B-spline of order d only depends on d + 1 points. The advantage of B-splines is a more stable behaviour under small changes of the input instruments for the curve construction.</li><li>Tension splines: attempt to dampen the &#x201C;wavy&#x201D; behaviour of interpolation functions even further, by forming a weighted mean of a piecewise linear and a cubic spline interpolation function.</li></ul><h2 id=interpolation-tools>Interpolation Tools</h2><h3 id=hyman-filter>Hyman Filter</h3><p>Hyman&#x2019;s monotonicity constraint filter ensures that in the regions of local monotoniticity of the input (three successive increasing or decreasing values) the interpolating cubic remains monotonic. If the interpolating cubic is already monotonic, the Hyman filter leaves it unchanged.</p><p>In the case of <span class="math inline">C^2</span> interpolants the Hyman filter ensures local monotonicity at the expense of the second derivative of the interpolant which will no longer be continuous in the points where the filter has been applied. While some non-linear schemes (Modified Parabolic, Fritsch-Butland, Kruger) are guaranteed to be locally monotone in their original approximation, all other schemes must be filtered according to the Hyman criteria at the expense of their linearity.</p><h1 id=numerical-differentiation>Numerical Differentiation</h1><p>For some complicated functions, it&#x2019;s not so easy to write down the derivative. We can numerically evaluate the derivative however.</p><h2 id=finite-differences>Finite Differences</h2><p>The derivative of a function <span class="math inline">f(x)</span> is defined as <span class="math display">f&#39;(x) = \lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}</span></p><p>Three simple finite difference formula are:</p><ul><li>forward: <span class="math inline">f&#39;(x) \approx \frac{f(x+h)-f(x)}{h}</span></li><li>backward: <span class="math inline">f&#39;(x) \approx \frac{f(x)-f(x-h)}{h}</span></li><li>central: <span class="math inline">f&#39;(x) \approx \frac{f(x+h)-f(x-h)}{2h}</span></li></ul><p>For the second derivative, one could use: <span class="math display">f&#39;&#39;(x) \approx \frac{f&#39;(x+h/2)-f&#39;(x-h/2)}{h} = \frac{f(x+h)+f(x-h)-2f(x)}{h^2}</span></p><p>There are more systematic methods for deriving finite difference formualas with Taylor series.</p><h2 id=lagrange-interpolation>Lagrange Interpolation</h2><h2 id=partial-differentiation>Partial Differentiation</h2><h1 id=numerical-integration>Numerical Integration</h1><p>Numerical integration is useful for integrals that don&#x2019;t have analytic formulas or are too cumbersome to do by hand.</p><h2 id=rectangular>Rectangular</h2><p>This is simply a Riemann sum. One can use the left, mid or right endpoints of the intervals to evaluate the function.</p><h2 id=trapezium-rule>Trapezium Rule</h2><p>The area of a trapezium specified by the points <span class="math inline">(a,f(a))</span> and <span class="math inline">(b,f(b))</span> is <span class="math display">T(a,b) = \frac{1}{2}(b-a)(f(a) + f(b)).</span></p><p><span class="math display"> \int_a^b f(x) dx = \sum_{i=1}^{n} \int_{x_{i-1}}^{x_i} f(x) dx \approx \sum_{i=1}^{n} T(x_{i-1},x_i).</span> If the <span class="math inline">x</span>-values are evenly-spaced, with <span class="math inline">h=(b-a)/n</span> we get <span class="math display"> \int_a^b f(x) dx \approx h\left( f(a)/2 + \sum_{i=2}^{n} f(x_j) + f(b)/2 \right),</span> where <span class="math inline">x_j = a+(j-1)h</span>.</p><p>The trapezium rule converges with order 2, <span class="math inline">O(h^2)</span>.</p><h2 id=simpsons-rule>Simpson&#x2019;s Rule</h2><p>Takes three points <span class="math inline">(a,f(a)), (b,f(b)), (c,f(c))</span> and fits a parabola through it. Uses the integral of the parabola <span class="math inline">P(x)</span><span class="math display"> \int_a^b f(x) dx \approx \int_a^b P(x) dx</span></p><p>With a step size <span class="math inline">h=(b-a)/2</span>, we get <span class="math display">\int_{a}^{b} P(x) \, dx =\tfrac{h}{3}\left[f(a) + 4f\left(\tfrac{a+b}{2}\right)+f(b)\right].</span> This is the Simpson&#x2019;s <span class="math inline">\tfrac{1}{3}</span>-rule.</p><p>A <em>composite</em> Simpson rule is obtained by applying the basic rule across pairs of intervals. Assumes an <em>even</em> number of intervals. <span class="math display">\int_{a}^{b} P(x) \, dx =\tfrac{h}{3}\left[f(a) + 4 \sum_{j=2,4,\ldots}^{N}f(x_j) + 2 \sum_{j=3,5,\ldots}^{N-1}f(x_j) +f(b)\right].</span></p><p>Simpson&#x2019;s rule converges with order 4, <span class="math inline">O(h^4)</span>.</p><h3 id=adaptive-simpsons-method>Adaptive Simpson&#x2019;s Method</h3><p>Uses an estimate of the error from calculating the definite integral using Simpson&#x2019;s rule. If the error exceeds a specified tolerance, the interval of integration is subdivided in two and adaptive Simpson&#x2019;s method is recursively called on each subinterval.</p><p>Since composite Simpson&#x2019;s will subdivide even in places where the function is well-approximated by a parabola, the adaptive Simpson&#x2019;s method can be much more efficient since it will use fewer function evaluations.</p><h2 id=gauss-integration>Gauss Integration</h2><p>The <span class="math inline">n</span>-point Gauss rule is defined as <span class="math display">\int_{-1}^{1} f(x) dx \approx \sum_{i=1}^{n} c_i f(x_i),</span> where <span class="math inline">c_i</span> are the weights and the <span class="math inline">x_i</span> are the nodes. The <span class="math inline">2n</span> weights and nodes need to be estimated. To estimate these, we require that the rule integrates <span class="math display">f(x)=x^{j},</span> exactly on <span class="math inline">[-1,1]</span> for <span class="math inline">j = 0,...,2n-1</span>. The <span class="math inline">x_i</span> are zeros of the Legendre polynomials, and thus this integration technique is also called Gauss-Legendre quadrature.</p><p>For example, the 2-point Gauss rule is defined as <span class="math display">\int_{-1}^{1} f(x) dx \approx f\left(\tfrac{-1}{\sqrt{3}}\right) + f\left(\tfrac{1}{\sqrt{3}}\right).</span> The 3-point Gauss rule is defined as <span class="math display">\int_{-1}^{1} f(x) dx \approx \tfrac{5}{9} f\left(-\sqrt{\tfrac{3}{5}}\right) + \tfrac{8}{9}f\left(0\right) + \tfrac{5}{9}f\left(\sqrt{\tfrac{3}{5}}\right).</span></p><p>An integral not on <span class="math inline">[-1,1]</span> can be tranformed into <span class="math display">\int_{a}^{b} f(x) dx =\int_{-1}^{1} f(t) dt,</span> by substituting <span class="math display">x=\frac{a+b+t(b-a)}{2},</span> with <span class="math inline">dx = \frac{b-a}{2} dt</span></p><h3 id=gausskronrod-quadrature>Gauss&#x2013;Kronrod Quadrature</h3><p>An adaptive variant of Gaussian quadrature, in which the evaluation points are chosen so that a more accurate approximation can be computed by re-using the the evaluation points from the previous iteration. It is an example of what is called a nested quadrature rule: for the same set of function evaluation points, it has two quadrature rules, one higher order and one lower order (the latter called an embedded rule). The difference between these two approximations is used to estimate the calculational error of the integration.</p><p>If the interval <span class="math inline">[a, b]</span> is subdivided, the usual Gauss evaluation points of the new subintervals never coincide with the previous evaluation points (except at the midpoint for odd numbers of evaluation points), and thus the integrand must be evaluated at every point. Gauss&#x2013;Kronrod formulas are extensions of the Gauss quadrature formulas generated by adding <span class="math inline">n+1</span> points to an <span class="math inline">n</span>-point rule in such a way that the resulting rule is of order <span class="math inline">3n+1</span>; the corresponding Gauss rule is of order <span class="math inline">2n-1</span>.</p><h2 id=clenshawcurtis-or-fejer-quadrature>Clenshaw&#x2013;Curtis or Fejer Quadrature</h2><p>Employs a change of variable <span class="math inline">x=\cos \theta</span> and uses a discrete cosine transform (DCT) approximation for the cosine series <span class="math display">\int_{-1}^1 f(x)\,dx=\int_{0}^{\pi }f(\cos\theta )\sin(\theta )\,d\theta</span><span class="math display">f(\cos \theta )=\frac{a_0}{2} + \sum_{k=1}^\infty a_k \cos(k\theta)</span></p><ul><li>has fast-converging accuracy comparable to Gaussian quadrature rules</li><li>naturally leads to nested quadrature rules, which is important for both adaptive quadrature and multidimensional quadrature (cubature).</li></ul><p>Equivalently, it is based on an expansion of the integrand in terms of Chebyshev polynomials. The function <span class="math inline">f(x)</span> to be integrated is evaluated at the <span class="math inline">N</span> extrema or roots of a Chebyshev polynomial. These values are used to construct a polynomial approximation for the function. This polynomial is then integrated exactly. In practice, the integration weights for the value of the function at each node are precomputed, and this computation can be performed in <span class="math inline">O(N\log N)</span> time by means of FFT-related algorithms for the DCT.</p><h1 id=solving-ordinary-differential-equations>Solving Ordinary Differential Equations</h1><h2 id=initial-value-problems>Initial Value Problems</h2><p>Problem:</p><p>Find the function <span class="math inline">y(x)</span> such <span class="math display">y&#39;(x)=f(x,y)</span> subject to the initial condition <span class="math inline">y_0 = y(x_0)</span> for a given function <span class="math inline">f</span>.</p><h3 id=eulers-method>Euler&#x2019;s Method</h3><p><span class="math display">y(x_{i+1}) \approx y(x_i) + hf(x_i,y(x_i)),</span> for <span class="math inline">i=0,1,2,\ldots</span>. This is a first order method, or Error = <span class="math inline">O(h)</span>.</p><h3 id=implicit-trapezium-rule>Implicit Trapezium Rule</h3><p><span class="math display">y(x_{i+1}) \approx y(x_i) + \tfrac{h}{2}\left( f(x_i,y(x_i)) + f(x_{i+1},y(x_{i+1})) \right).</span> This method requires us to know <span class="math inline">y(x_{i+1})</span> before estimating <span class="math inline">y(x_{i+1})</span>, which is a problem. We can try solve for this analytically or (more usually) numerically, or we could use an estimate for the <span class="math inline">y(x_{i+1})</span> on the right using the Euler method.</p><h3 id=modified-eulers-method>Modified Euler&#x2019;s Method</h3><p>We estimate <span class="math inline">y(x_{i+1})</span> from the implicit trapezium rule using one step of Euler&#x2019;s method. <span class="math display">y(x_{i+1})^E = y(x_i) + hf(x_i,y(x_i)),</span><span class="math display">y(x_{i+1}) \approx y(x_i) + \tfrac{h}{2}\left( f(x_i,y(x_i)) + f(x_{i+1},y(x_{i+1})^E) \right).</span> This is a second order method, or Error = <span class="math inline">O(h^2)</span>.</p><h3 id=midpoint-method>Midpoint Method</h3><p><span class="math display">y(x_{i+0.5})^E = y(x_i) + \tfrac{h}{2}f(x_i,y(x_i))</span><span class="math display">y(x_{i+1}) \approx y(x_i) + \tfrac{h}{2} f(x_i+\tfrac{h}{2},y(x_{i+0.5})^E).</span></p><p>The above are low order members of the Runge-Kutta methods.</p><p><code>ode45</code> in Matlab.</p><h2 id=boundary-value-problems>Boundary Value Problems</h2><p>Problem:</p><p>Find the function <span class="math inline">y(x)</span> such <span class="math display">y&#39;&#39;(x) + f(x)y&#39;(x) + g(x)y = h(x),</span> for <span class="math inline">a &lt; x &lt; b</span>, subject to boundary conditions <span class="math inline">y(a)=y_a, y(b)=y_b</span>.</p><p>Subdivide the interval <span class="math inline">[a,b]</span> into smaller intervals of width <span class="math inline">h</span>. Use central difference approximations of the derivatives <span class="math inline">y&#39;, y&#39;&#39;</span>. Once the set of equations are simplified, it results in a tridiagonal system.</p><h1 id=solving-partial-differential-equations>Solving Partial Differential Equations</h1><p>Problem:</p><p>Find the function <span class="math inline">u(x,y)</span> such that, for example: <span class="math display">u_{xx}+u_{yy} = 0,</span> subject to some boundary conditions.</p><p>A two-variable PDE can be solved by using a derivative stencil (like the 5-point stencil) on a finite-difference grid. This produces a large sparse matrix which can be solved using different techniques:</p><ul><li>direct methods</li><li>iterative methods: e.g.&#xA0;Gauss-Seidel method, conjugate gradient method. These are better when the systems are huge, or only a low accuracy is needed.</li></ul><h1 id=references>References</h1><ul><li>Interpolation <ul><li><a href=https://www.cs.cornell.edu/~bindel/class/cs3220-s12/notes/lec19.pdf class=uri>https://www.cs.cornell.edu/~bindel/class/cs3220-s12/notes/lec19.pdf</a></li><li><a href=https://www.mathworks.com/help/matlab/ref/pchip.html class=uri>https://www.mathworks.com/help/matlab/ref/pchip.html</a></li></ul></li></ul></article></div></main><footer class="site-footer h-card"><data href=/ class=u-url></data><div class=wrapper><h2 class=footer-heading>Notes in Progress</h2><div class=footer-col-wrapper><div class="footer-col footer-col-1"><ul class=contact-list><li class=p-name>febkor</li></ul></div></div></div></div></footer></body></html>